{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c2d190-7adc-4643-af63-f89cf68244f3",
   "metadata": {},
   "source": [
    "# Brief Intro to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f5b8a-11cd-459a-8d6d-724dea01323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time, warnings, textwrap, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "try:\n",
    "    from scipy.special import kv, kvp   # Bessel K(v, x) and ∂/∂v K(v, x)\n",
    "    SCIPY_OK = True\n",
    "except Exception as e:\n",
    "    SCIPY_OK = False\n",
    "    warnings.warn(\"SciPy not found. Custom Bessel example will be CPU-only or skipped.\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"PyTorch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21ce80-54d6-4f37-bd9f-a0bce848a8e6",
   "metadata": {},
   "source": [
    "## 1) Why autograd (vs NumPy) & backprop\n",
    "\n",
    "NumPy is great for arrays but **doesn't track computation graphs**. If you want gradients, you must do calculus by hand.  \n",
    "PyTorch **records ops**, so calling `.backward()` gives you **exact vector–Jacobian products** using optimized backprop.\n",
    "\n",
    "Two key advantages:\n",
    "\n",
    "**1. Automatic Differentiation (Autograd)**\n",
    "- In NumPy, you have to hand-code derivatives. This is slow, error-prone, and hard to maintain.\n",
    "- In PyTorch, the computation graph is recorded automatically. Calling `.backward()` computes gradients *exactly* with efficient backpropagation.\n",
    "- This means we can train deep models with millions of parameters without ever touching calculus.\n",
    "\n",
    "**2. Optimized GPU Kernels**\n",
    "- PyTorch uses high-performance backends (MKL, cuBLAS, cuDNN).  \n",
    "- Matrix multiplies, convolutions, and reductions run *orders of magnitude* faster than naive NumPy on CPU.  \n",
    "- On GPU, PyTorch can be 10–100× faster than NumPy on the same task.  \n",
    "\n",
    "Together: PyTorch makes it possible to write **concise, research-friendly code** that is also **fast enough for production training**.\n",
    "\n",
    "\n",
    "Below: the same tiny regression loss in NumPy (manual grad) vs PyTorch (autograd).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7035209-7393-4496-9341-f4fd96a71cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy: y_hat = w * x ; loss = (y_hat - y)^2\n",
    "\n",
    "# NumPy (manual gradient)\n",
    "x_np = np.array([1.0, 2.0, 3.0])\n",
    "y_np = np.array([2.0, 4.1, 5.9])\n",
    "w_np = 0.0\n",
    "\n",
    "# forward\n",
    "yhat_np = w_np * x_np\n",
    "loss_np = np.mean((yhat_np - y_np)**2)\n",
    "\n",
    "# manual grad dL/dw = 2 * mean((w*x - y) * x)\n",
    "grad_w_np = 2.0 * np.mean((yhat_np - y_np) * x_np)\n",
    "\n",
    "print(f\"[NumPy] loss={loss_np:.4f}, grad_w={grad_w_np:.4f}\")\n",
    "\n",
    "# PyTorch (autograd)\n",
    "x = torch.tensor(x_np, dtype=torch.float32)\n",
    "y = torch.tensor(y_np, dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "yhat = w * x\n",
    "loss = torch.mean((yhat - y)**2)\n",
    "loss.backward()\n",
    "print(f\"[Torch]  loss={loss.item():.4f}, grad_w={w.grad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d7b5f-35dc-4fb2-93f2-d61ea5383f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, time\n",
    "\n",
    "# -------------------------\n",
    "# Example 1: gradient by hand vs autograd\n",
    "# -------------------------\n",
    "x_np = np.linspace(-2, 2, 100)\n",
    "w_np = 3.0\n",
    "y_np = w_np * x_np**2\n",
    "\n",
    "# manual gradient of loss wrt w\n",
    "# loss = mean(w * x^2)\n",
    "grad_w_np = np.mean(x_np**2)\n",
    "print(f\"[NumPy manual] grad wrt w = {grad_w_np:.4f}\")\n",
    "\n",
    "# PyTorch version\n",
    "x = torch.linspace(-2, 2, 100, dtype=torch.float32)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "y = w * x**2\n",
    "loss = y.mean()\n",
    "loss.backward()\n",
    "print(f\"[PyTorch autograd] grad wrt w = {w.grad.item():.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Example 2: performance (matrix multiply)\n",
    "# -------------------------\n",
    "N = 2000\n",
    "\n",
    "A_np = np.random.randn(N, N).astype(np.float32)\n",
    "B_np = np.random.randn(N, N).astype(np.float32)\n",
    "\n",
    "A_torch = torch.from_numpy(A_np)\n",
    "B_torch = torch.from_numpy(B_np)\n",
    "\n",
    "# NumPy\n",
    "t0 = time.time()\n",
    "C_np = A_np @ B_np\n",
    "t1 = time.time()\n",
    "print(f\"[NumPy] {t1 - t0:.3f} sec\")\n",
    "\n",
    "# PyTorch\n",
    "t0 = time.time()\n",
    "C_torch = A_torch @ B_torch\n",
    "t1 = time.time()\n",
    "print(f\"[PyTorch] {t1 - t0:.3f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5a2dc-3ab1-4320-8449-bc363f5ab896",
   "metadata": {},
   "source": [
    "## 2) Torch tensors: creation, shapes, dtypes, devices\n",
    "\n",
    "Key tips:\n",
    "- Keep **model & inputs on the same device** (`cpu` or `cuda`).\n",
    "- Move **once per step** (avoid ping-ponging between CPU/GPU).\n",
    "- Use `.to(device)` or `.cuda()` / `.cpu()`.\n",
    "- Prefer **float32** (training) or **bfloat16**/**float16** with **AMP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48318254-0c44-46b9-becd-be02e7fec51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation\n",
    "a = torch.zeros((2, 3), dtype=torch.float32)\n",
    "b = torch.ones_like(a)\n",
    "c = torch.rand((2, 3))                  # U[0,1)\n",
    "d = torch.randn((3, 3))                 # N(0,1)\n",
    "e = torch.arange(0, 10).reshape(2, 5)\n",
    "\n",
    "# shape/dtype/device\n",
    "print(a.shape, a.dtype, a.device)\n",
    "print(f'a is {a}')\n",
    "print(f'b is {b}')\n",
    "print(f'c is {c}')\n",
    "print(f'd is {d}')\n",
    "print(f'e is {e}')\n",
    "\n",
    "# device moves\n",
    "a_gpu = a.to(device)\n",
    "d_gpu = d.to(device, non_blocking=True)  # non_blocking requires pinned CPU memory when coming from DataLoader\n",
    "\n",
    "# simple math\n",
    "z = torch.matmul(d_gpu, torch.eye(3, device=device))\n",
    "print(f'z is {z}')\n",
    "print(\"z device:\", z.device, \"z shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95144361-2a5c-4227-9385-f960e97caeda",
   "metadata": {},
   "source": [
    "## 3) Tracking autograd vs not\n",
    "\n",
    "- `requires_grad=True` starts tracking.\n",
    "- **Default**: tensors created from ops on tracked tensors also track.\n",
    "- Turn off with:\n",
    "  - `with torch.no_grad():` (training or inference; faster, saves memory)\n",
    "  - `with torch.inference_mode():` (inference only; even leaner)\n",
    "- Break gradient flow with `.detach()`.\n",
    "- Non-scalar losses need `loss.backward(gradient=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20db69-3e58-4e7a-a6d4-1e3a21b57a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "x = torch.tensor(3.0)\n",
    "\n",
    "# Tracks: (w * x)^2\n",
    "y = (w * x) ** 2\n",
    "y.backward()\n",
    "print(\"dw after backward:\", w.grad.item())\n",
    "\n",
    "# Stop tracking\n",
    "with torch.no_grad():\n",
    "    y2 = (w * x) ** 2  # no graph\n",
    "# or\n",
    "y3 = (w.detach() * x) ** 2  # explicitly breaks graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a96e0c-ad3f-4dc5-ae58-2f11cf912164",
   "metadata": {},
   "source": [
    "The Difference between `detach()` vs `item()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9775a54-5a37-46b9-b04c-2dcfe692634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a scalar tensor that requires gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x\n",
    "\n",
    "# Case 1: .item()\n",
    "# ----------------\n",
    "# .item() extracts a standard Python number (float) from a one-element tensor.\n",
    "# Useful for logging/printing, but breaks the computation graph (no gradients).\n",
    "print(\"Using .item():\")\n",
    "print(\"y as Python number:\", y.item())\n",
    "print(\"Type:\", type(y.item()))\n",
    "\n",
    "# Case 2: .detach()\n",
    "# -----------------\n",
    "# .detach() creates a new tensor that shares the same data\n",
    "# but is disconnected from the computation graph.\n",
    "# Useful if you want the tensor for further computation, but without gradient tracking.\n",
    "z = y.detach()\n",
    "print(\"\\nUsing .detach():\")\n",
    "print(\"z (detached tensor):\", z)\n",
    "print(\"z requires_grad?\", z.requires_grad)\n",
    "\n",
    "# Demonstrating difference in backprop\n",
    "y.backward()  # backprop works because y is connected to x\n",
    "print(\"\\nGradient in x.grad after y.backward():\", x.grad)\n",
    "\n",
    "try:\n",
    "    z.backward()\n",
    "except RuntimeError as e:\n",
    "    print(\"\\nBackprop through detached tensor fails:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bff764-ab39-41d5-abac-b68224ffc812",
   "metadata": {},
   "source": [
    "## 4) When standard ops aren't enough: Custom `autograd.Function`\n",
    "\n",
    "Example: **Matérn kernel** uses the modified Bessel function of the second kind \\(K_\\nu\\), which PyTorch doesn't natively expose.  \n",
    "We can **wrap SciPy** (CPU) for the forward and supply our **own backward**. Caveats:\n",
    "\n",
    "- Keep **dtype/device** consistent (we'll use `float64` for stability).\n",
    "- Moving to NumPy implies CPU; be explicit on device transfers.\n",
    "- Numerical finite-differences for gradients are slower/approximate → good for correctness, not for production speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a7013-5af5-4702-bc59-b8e24d396d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_positive_definite(matrix: torch.Tensor) -> bool:\n",
    "    \"\"\"Check if a matrix is positive definite.\"\"\"\n",
    "    try:\n",
    "        torch.linalg.cholesky(matrix)\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        return False\n",
    "\n",
    "if not SCIPY_OK:\n",
    "    warnings.warn(\"SciPy not available: skipping custom BesselKFunction demonstration.\")\n",
    "\n",
    "class BesselKFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, v: torch.Tensor, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        v, x: float64 tensors (can be broadcastable).\n",
    "        We'll compute on CPU via SciPy, then return to x.device.\n",
    "        \"\"\"\n",
    "        if not SCIPY_OK:\n",
    "            raise RuntimeError(\"SciPy is required for this example.\")\n",
    "\n",
    "        # Save for backward (save tensors as-is; do not detach here)\n",
    "        ctx.save_for_backward(v, x)\n",
    "\n",
    "        # Move to CPU numpy\n",
    "        v_cpu = v.detach().cpu().numpy()\n",
    "        x_cpu = x.detach().cpu().numpy()\n",
    "\n",
    "        out = kv(v_cpu, x_cpu)  # SciPy: elementwise modified Bessel K\n",
    "        out_t = torch.from_numpy(np.asarray(out)).to(dtype=torch.float64, device=x.device)\n",
    "        return out_t\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        v, x = ctx.saved_tensors\n",
    "        if not SCIPY_OK:\n",
    "            raise RuntimeError(\"SciPy is required for this example.\")\n",
    "\n",
    "        # Finite-diff wrt x\n",
    "        eps_x = 1e-12\n",
    "        v_cpu = v.detach().cpu().numpy()\n",
    "        x_cpu = x.detach().cpu().numpy()\n",
    "\n",
    "        k_plus  = kv(v_cpu, x_cpu + eps_x)\n",
    "        k_minus = kv(v_cpu, x_cpu - eps_x)\n",
    "        dK_dx = (k_plus - k_minus) / (2 * eps_x)\n",
    "\n",
    "        # Exact derivative wrt v via kvp (SciPy)\n",
    "        dK_dv = kvp(v_cpu, x_cpu)\n",
    "\n",
    "        dK_dx_t = torch.from_numpy(np.asarray(dK_dx)).to(dtype=torch.float64, device=x.device)\n",
    "        dK_dv_t = torch.from_numpy(np.asarray(dK_dv)).to(dtype=torch.float64, device=v.device)\n",
    "\n",
    "        # Chain rule\n",
    "        grad_v = grad_output * dK_dv_t\n",
    "        grad_x = grad_output * dK_dx_t\n",
    "        return grad_v, grad_x  # matches (v, x) order\n",
    "\n",
    "def matern_kernel(pairwise_distances: torch.Tensor,\n",
    "                  length_scale: torch.Tensor,\n",
    "                  nu: torch.Tensor,\n",
    "                  sigma: torch.Tensor,\n",
    "                  epsilon: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Matérn covariance with broadcasting support.\n",
    "    All inputs should be float64 to match our Bessel implementation.\n",
    "    \"\"\"\n",
    "    # ensure float64 for stability with special functions\n",
    "    dtype = torch.float64\n",
    "    P = pairwise_distances.to(dtype)\n",
    "    L = length_scale.to(dtype)\n",
    "    N = nu.to(dtype)\n",
    "    S = sigma.to(dtype)\n",
    "    sigma2 = S**2\n",
    "\n",
    "    # avoid exactly 0.5 to dodge special-case branches\n",
    "    N = torch.where(N == 0.5, N + epsilon, N)\n",
    "\n",
    "    # scaled distances\n",
    "    scaled = torch.sqrt(2.0 * N) * (P / L)\n",
    "    # clamp for numeric stability\n",
    "    scaled = torch.clamp(scaled, min=1e-10, max=1e6)\n",
    "\n",
    "    # Bessel term (custom autograd)\n",
    "    K = BesselKFunction.apply(N, scaled)\n",
    "\n",
    "    # front factor: 2^{1-ν} / Γ(ν)\n",
    "    scaling = (2.0 ** (1.0 - N)) / torch.exp(torch.lgamma(N))\n",
    "\n",
    "    cov = sigma2 * scaling * (scaled ** N) * K\n",
    "    # exact diag at zero distance\n",
    "    cov = torch.where(P == 0, sigma2, cov)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2a6d9-0795-47b1-8a4b-cd34ab9525a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Demo: using the custom BesselKFunction inside the Matérn kernel\n",
    "\n",
    "# Small set of \"locations\"\n",
    "X = torch.linspace(0, 1, 5, dtype=torch.float64)\n",
    "pairwise_dist = torch.cdist(X.unsqueeze(1), X.unsqueeze(1))\n",
    "\n",
    "# Parameters (require gradients so we can test backprop)\n",
    "length_scale = torch.tensor(0.2, dtype=torch.float64, requires_grad=True)\n",
    "nu          = torch.tensor(0.9, dtype=torch.float64, requires_grad=True)\n",
    "sigma       = torch.tensor(1.0, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Build covariance matrix\n",
    "K = matern_kernel(pairwise_dist, length_scale, nu, sigma)\n",
    "print(\"Covariance matrix:\\n\", K)\n",
    "print(\"Positive definite?\", is_positive_definite(K))\n",
    "\n",
    "# Toy \"loss\" = log determinant + trace (GP-style terms)\n",
    "loss = torch.logdet(K) + torch.trace(K)\n",
    "print(\"Loss value:\", loss.item())\n",
    "\n",
    "# Backward: should give gradients wrt hyperparams\n",
    "loss.backward()\n",
    "print(\"dL/d length_scale:\", length_scale.grad.item())\n",
    "print(\"dL/d nu:\", nu.grad.item())\n",
    "print(\"dL/d sigma:\", sigma.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f7aa9-0795-4a0b-af42-4a68fcded0f7",
   "metadata": {},
   "source": [
    "## 5) Optimization strategies\n",
    "\n",
    "- Separate **learning rates** per parameter group when scales differ.  \n",
    "- **Early stopping** with patience.  \n",
    "- **Reduce LR on plateau** (or manual reduction).  \n",
    "- **Gradient clipping** to tame exploding grads.  \n",
    "- **Anomaly detection** while debugging.\n",
    "\n",
    "Below uses **Adagrad** with different per-param LRs (as in your snippet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55926d72-55b0-46f5-b3b0-7bcd063b9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scalar params to fit (toy)\n",
    "alpha_i = torch.tensor(0.1,  dtype=torch.float64, requires_grad=True)\n",
    "nu_i    = torch.tensor(0.9,  dtype=torch.float64, requires_grad=True)\n",
    "sigma_i = torch.tensor(1.0,  dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adagrad([\n",
    "    {'params': [alpha_i], 'lr': 0.0003},\n",
    "    {'params': [nu_i],    'lr': 0.008},\n",
    "    {'params': [sigma_i], 'lr': 0.25},\n",
    "], lr_decay=0, weight_decay=0, eps=1e-15)\n",
    "\n",
    "# Optional: scheduler (manual below, but here's a standard one)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "tolerance = 1e-15\n",
    "patience = 25\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "max_lr_reductions = 3\n",
    "reductions = 0\n",
    "\n",
    "gradient_clip = 5.0\n",
    "number_of_cycles = 100\n",
    "steps_per_batch = 5\n",
    "\n",
    "# Dummy batches (replace with your X_groups, Y_groups)\n",
    "X_groups = [torch.linspace(0, 1, 50, dtype=torch.float64)]\n",
    "Y_groups = [torch.sin(X_groups[0])]\n",
    "\n",
    "def toy_nll(alpha, nu, sigma, X, Y):\n",
    "    # This is a stand-in for your Matérn-based NLL.\n",
    "    # Replace with your real loss.\n",
    "    pred = sigma * torch.exp(-alpha * (X - 0.5)**2)\n",
    "    return torch.mean((pred - Y)**2) + 0.01*(alpha**2 + nu**2 + sigma**2)\n",
    "\n",
    "for epoch in range(number_of_cycles):\n",
    "    total_loss = 0.0\n",
    "    for Xb, Yb in zip(X_groups, Y_groups):\n",
    "        for _ in range(steps_per_batch):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = toy_nll(alpha_i, nu_i, sigma_i, Xb, Yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_([alpha_i, nu_i, sigma_i], gradient_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(X_groups) * steps_per_batch)\n",
    "    improved = avg_loss + tolerance < best_loss\n",
    "    if improved:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Manual LR reduce on plateau\n",
    "    if epochs_no_improve >= patience and reductions < max_lr_reductions:\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg['lr'] *= 0.1\n",
    "        reductions += 1\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "    # scheduler.step(avg_loss)  # if using ReduceLROnPlateau\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch:03d} | loss {avg_loss:.6e} | lr {[pg['lr'] for pg in optimizer.param_groups]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623f36-fef8-449b-bb29-13830eeb748d",
   "metadata": {},
   "source": [
    "## 6) DTypes under GPU constraints\n",
    "\n",
    "Common training defaults:\n",
    "- **float32** (safe)\n",
    "- **bfloat16 / float16 + AMP** (Automatic Mixed Precision) → faster & less memory\n",
    "- Model weights often **float16/bfloat16**, activations **float16/bfloat16**, **master weights** kept in float32 (optimizers).\n",
    "\n",
    "For **very tight memory** (many teams do this):\n",
    "- **int8 / int4 weight quantization** for *inference*\n",
    "- **activation checkpointing** to trade compute for memory\n",
    "- **gradient accumulation** to simulate larger batch sizes\n",
    "- **bf16** preferred over fp16 on modern GPUs (better range, no loss scaling)\n",
    "\n",
    "Below: AMP training pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b97b6-8904-48c9-b519-8aed08359526",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 10)).to(device)\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X = torch.randn(256, 32, device=device)\n",
    "y = torch.randint(0, 10, (256,), device=device)\n",
    "\n",
    "for step in range(10):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(opt)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(opt)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf25fa6-42f7-4cb5-84e4-953c08d9574d",
   "metadata": {},
   "source": [
    "## 7) Debugging mode\n",
    "\n",
    "- `torch.autograd.set_detect_anomaly(True)` pinpoints the exact bad op (slow; use temporarily).\n",
    "- Catch **NaN/Inf** with hooks or checks.\n",
    "- Use `gradcheck` for custom ops.\n",
    "- Profile with `torch.autograd.profiler` or `torch.profiler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5039e-089d-4993-a348-6e7f9315296c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)  # SLOW — only enable while debugging\n",
    "\n",
    "def nan_hook(grad):\n",
    "    if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "        print(\"Detected NaN/Inf in gradient!\")\n",
    "    return grad\n",
    "\n",
    "# Attach to a parameter for demo\n",
    "param = nn.Parameter(torch.randn(()), requires_grad=True)\n",
    "param.register_hook(nan_hook)\n",
    "\n",
    "loss = (param * float('nan'))**2  # force NaN\n",
    "try:\n",
    "    loss.backward()\n",
    "except Exception as e:\n",
    "    print(\"Backward failed (as expected in demo):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e2e5c-7d73-4d7b-9685-932ee69a4c62",
   "metadata": {},
   "source": [
    "## 8) Estimating time\n",
    "\n",
    "- Measure **avg batch time** × remaining batches → ETA.\n",
    "- Use a lightweight **moving average**.\n",
    "- Add a **tqdm** progress bar if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ecc01-6ed1-4202-bed1-fd7375361bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque # doubled ended queue which automatically drops the oldest items\n",
    "\n",
    "def run_with_eta(n_steps=200, window=20):\n",
    "    times = deque(maxlen=window)\n",
    "    t0 = time.perf_counter()\n",
    "    for step in range(1, n_steps + 1):\n",
    "        # simulate work\n",
    "        time.sleep(0.005)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "        t0 = t1\n",
    "\n",
    "        avg = sum(times) / len(times)\n",
    "        remaining = n_steps - step\n",
    "        eta_s = remaining * avg\n",
    "        if step % 25 == 0 or step == n_steps:\n",
    "            print(f\"step {step}/{n_steps} | avg_step {avg*1000:.2f} ms | ETA {eta_s:.2f} s\")\n",
    "\n",
    "run_with_eta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155eec6-b1af-4384-bb03-270060b14abd",
   "metadata": {},
   "source": [
    "## 9) Conversion: NumPy, Pandas, Torch\n",
    "\n",
    "- `torch.from_numpy(np_array)` (shares memory; CPU only)\n",
    "- `tensor.numpy()` (CPU only; shares memory)\n",
    "- For GPU tensors, first `.cpu()`.\n",
    "- Pandas → Tensor: convert columns to NumPy, then `from_numpy`.\n",
    "- Zero-copy interop: **DLPack** (`torch.utils.dlpack`).\n",
    "\n",
    "Beware: **shared memory** means in-place changes reflect on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8713c93-a8d2-49d0-b3a0-01270fbad9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy -> Torch\n",
    "np_arr = np.random.randn(3, 4).astype(np.float32)\n",
    "t_from_np = torch.from_numpy(np_arr)         # shares memory (CPU)\n",
    "print(\"shares memory:\", t_from_np.data_ptr() == torch.from_numpy(np_arr).data_ptr())\n",
    "\n",
    "# Torch -> NumPy\n",
    "np_back = t_from_np.numpy()                  # zero-copy (CPU)\n",
    "t_gpu = torch.randn(3, 4, device=device)\n",
    "np_from_gpu = t_gpu.detach().cpu().numpy()   # copy\n",
    "\n",
    "# Pandas -> Torch\n",
    "df = pd.DataFrame({\"x\": np.random.randn(5), \"y\": np.random.randn(5)})\n",
    "X_t = torch.from_numpy(df.values).float()    # (5, 2)\n",
    "\n",
    "# DLPack (zero-copy advanced)\n",
    "capsule = torch.utils.dlpack.to_dlpack(t_from_np)\n",
    "t2 = torch.utils.dlpack.from_dlpack(capsule)  # shares storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cb401-7876-4b86-b97d-39e65249f1d2",
   "metadata": {},
   "source": [
    "## 10) Memory profiler & run tracking\n",
    "\n",
    "- CUDA memory: `torch.cuda.memory_allocated()`, `max_memory_allocated()`, `reset_peak_memory_stats()`.\n",
    "- Model summary: `torch.cuda.memory_summary()`.\n",
    "- Wall-clock: `time.perf_counter()`.\n",
    "- Lightweight experiment log: append CSV rows with config/metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbbd07-4467-48f3-bb32-1a3bf3cbf6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_snapshot(tag=\"snapshot\"):\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        alloc = torch.cuda.memory_allocated() / 1024**2\n",
    "        peak  = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"[{tag}] CUDA alloc={alloc:.1f} MB | peak={peak:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"[{tag}] CPU mode; use tracemalloc or psutil for details.\")\n",
    "\n",
    "# demo\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    a = torch.randn(1024, 1024, device=device)\n",
    "    memory_snapshot(\"after big tensor\")\n",
    "\n",
    "# simple CSV logger\n",
    "from datetime import datetime\n",
    "log_path = Path(\"runs_log.csv\")\n",
    "def log_run(**kwargs):\n",
    "    row = {\"timestamp\": datetime.now().isoformat(), **kwargs}\n",
    "    df = pd.DataFrame([row])\n",
    "    if log_path.exists():\n",
    "        df.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(log_path, index=False)\n",
    "    return row\n",
    "\n",
    "_ = log_run(model=\"demo-mlp\", lr=1e-3, best_loss=0.1234, epochs=10, notes=\"sanity check\")\n",
    "print(\"Logged to\", log_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7ef26-d38e-4e95-9ee2-6822c8f73a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6135c4-6fe1-46fb-90e9-6ce48c1b93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext memory_profiler\n",
    "\n",
    "# Allocate a big tensor\n",
    "%memit a = torch.randn(10000, 10000)\n",
    "\n",
    "# Free it\n",
    "del a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
