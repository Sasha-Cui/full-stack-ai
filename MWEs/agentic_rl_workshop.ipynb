{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Reinforcement Learning: From Traditional RL to Autonomous Agents\n",
    "\n",
    "**Workshop Overview:**\n",
    "This workshop introduces the emerging field of Agentic Reinforcement Learning, where large language models (LLMs) are transformed into autonomous agents through RL training.\n",
    "\n",
    "\n",
    "**Key Papers:**\n",
    "- Wang et al. (2024). \"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\" [arXiv:2509.02547](https://arxiv.org/abs/2509.02547)\n",
    "- Yao et al. (2024). \"Demystifying Reinforcement Learning in Agentic Reasoning\" [arXiv:2510.11701](https://arxiv.org/abs/2510.11701)\n",
    "- Zhou et al. (2024). \"AgentRL: Scaling Agentic RL with Multi-Turn, Multi-Task Framework\" [arXiv:2510.04206](https://arxiv.org/abs/2510.04206)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: What is Agentic RL?\n",
    "\n",
    "### Traditional RL vs Agentic RL\n",
    "\n",
    "**Traditional RL** focuses on training agents to maximize rewards in well-defined environments:\n",
    "- Game playing (Chess, Go, Atari)\n",
    "- Robotic control\n",
    "- Resource optimization\n",
    "\n",
    "**Agentic RL** extends this to create autonomous agents that can:\n",
    "- **Reason** about complex problems\n",
    "- **Use tools** and external resources\n",
    "- **Plan** multi-step strategies\n",
    "- **Remember** and learn from past experiences\n",
    "- **Self-improve** through reflection\n",
    "\n",
    "### The Paradigm Shift\n",
    "\n",
    "```\n",
    "Traditional RL:           Agentic RL:\n",
    "State → Action            Observation → Thought → Tool Use → Action\n",
    "                                 ↓\n",
    "                            Memory, Planning, Reflection\n",
    "```\n",
    "\n",
    "**Why LLMs + RL?**\n",
    "- LLMs provide natural language understanding and reasoning\n",
    "- RL provides goal-directed behavior optimization\n",
    "- Together: Agents that can understand, reason, and act autonomously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:13.875375Z",
     "iopub.status.busy": "2025-12-02T22:46:13.875245Z",
     "iopub.status.idle": "2025-12-02T22:46:17.731540Z",
     "shell.execute_reply": "2025-12-02T22:46:17.730781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy matplotlib gymnasium -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"✓ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Traditional RL: Markov Decision Process\n",
    "\n",
    "Traditional RL models the world as a **Markov Decision Process (MDP)**:\n",
    "- The agent is in a **state**\n",
    "- Takes an **action**\n",
    "- Transitions to a **new state**\n",
    "- Receives a **reward**\n",
    "\n",
    "**Goal:** Find a policy (mapping from states → actions) that maximizes cumulative reward.\n",
    "\n",
    "$$\\text{MDP} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{T}, \\gamma)$$\n",
    "\n",
    "| Symbol | Meaning | Example |\n",
    "|--------|---------|--------|\n",
    "| $\\mathcal{S}$ | State space | Game pixels, board positions |\n",
    "| $\\mathcal{A}$ | Action space | Move left/right, place piece |\n",
    "| $\\mathcal{R}$ | Reward function | +1 win, -1 lose |\n",
    "| $\\gamma$ | Discount factor | 0.99 (value future rewards) |\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Agentic RL: What's Different?\n",
    "\n",
    "$$\\text{Agentic MDP} = (\\mathcal{O}, \\mathcal{C}, \\mathcal{T}_{tool}, \\mathcal{M}, \\mathcal{A}, \\mathcal{R}, \\gamma)$$\n",
    "\n",
    "| New Component | What It Is | Example |\n",
    "|---------------|------------|--------|\n",
    "| $\\mathcal{O}$ (Observations) | Natural language input | \"What is 5+3?\" |\n",
    "| $\\mathcal{C}$ (Cognitive Actions) | Thinking, planning | \"I need to calculate...\" |\n",
    "| $\\mathcal{T}_{tool}$ (Tools) | External functions | Calculator, search, code |\n",
    "| $\\mathcal{M}$ (Memory) | Past interactions | Previous conversation |\n",
    "\n",
    "**The key innovation: Two types of reward**\n",
    "\n",
    "| Reward Type | When Given | Signal Density | Example |\n",
    "|-------------|------------|----------------|--------|\n",
    "| **Process Reward** | Each reasoning step | Dense | +0.3 for using calculator correctly |\n",
    "| **Outcome Reward** | Final answer only | Sparse | +1.0 for correct answer |\n",
    "\n",
    "**Process rewards are what make agentic RL work** — they provide learning signal for every reasoning step, not just the final answer!\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Comparison\n",
    "\n",
    "| Aspect | Traditional RL | Agentic RL |\n",
    "|--------|---------------|------------|\n",
    "| Input | Numbers (pixels) | Natural language |\n",
    "| Processing | state → action | observe → think → use tool → act |\n",
    "| Tools | None | Calculator, search, code |\n",
    "| Reward | Outcome only | **Process + Outcome** |\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Training Objective\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{RL} + \\lambda_1 \\mathcal{L}_{SFT} + \\lambda_2 \\mathcal{L}_{KL}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 The Three Training Losses\n",
    "\n",
    "---\n",
    "\n",
    "#### **Loss 1: RL Loss — Maximize Reward**\n",
    "\n",
    "$$\\mathcal{L}_{RL} = -\\mathbb{E} \\left[ \\sum_t \\log \\pi_\\theta(a_t | s_t) \\cdot A_t \\right]$$\n",
    "\n",
    "**What it does:** This is the policy gradient. If an action got better-than-expected reward (positive advantage), increase its probability. If worse, decrease it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $\\log \\pi_\\theta(a_t|s_t)$ | How confident the model was about this action |\n",
    "| $A_t$ | Advantage: was this action better or worse than average? |\n",
    "\n",
    "**Intuition:** Reinforce good actions, suppress bad ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Loss 2: SFT Loss — Teach Good Behavior**\n",
    "\n",
    "$$\\mathcal{L}_{SFT} = -\\log \\pi_\\theta(y | x)$$\n",
    "\n",
    "**What it does:** Supervised fine-tuning. Show the model examples of good behavior and train it to imitate them. This keeps the model coherent.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|--------|\n",
    "| $x$ | Input prompt (observation) |\n",
    "| $y$ | Expert response (what a good agent would say) |\n",
    "\n",
    "**Intuition:** \"Here's how to do it correctly\" — learn from expert demonstrations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Loss 3: KL Loss — Don't Forget**\n",
    "\n",
    "$$\\mathcal{L}_{KL} = D_{KL}(\\pi_\\theta \\| \\pi_{ref})$$\n",
    "\n",
    "**What it does:** Measures how different the current model is from the original base model. Keeps the model from drifting too far.\n",
    "\n",
    "| Model | Description |\n",
    "|-------|------------|\n",
    "| Base/Reference model | Knows everything, but bad at specific task |\n",
    "| Trained model | Good at task, but might forget other things |\n",
    "\n",
    "**Intuition:** \"Don't forget what you already know\" — stay grounded.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Combined: The Balancing Act**\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{\\mathcal{L}_{RL}}_{\\text{get rewards}} + \\lambda_1 \\underbrace{\\mathcal{L}_{SFT}}_{\\text{stay coherent}} + \\lambda_2 \\underbrace{\\mathcal{L}_{KL}}_{\\text{don't drift}}$$\n",
    "\n",
    "Typical weights: $\\lambda_1 = 0.1$, $\\lambda_2 = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Agentic Capabilities Taxonomy\n",
    "\n",
    "Based on Wang et al. (2024), agentic systems exhibit **six core capabilities**:\n",
    "\n",
    "### 3.1 Planning\n",
    "- **Definition:** Breaking down complex tasks into manageable sub-goals\n",
    "- **Examples:** Task decomposition, goal setting, strategy formulation\n",
    "- **RL Role:** Learn to generate effective plans through reward feedback\n",
    "\n",
    "### 3.2 Tool Use\n",
    "- **Definition:** Selecting and invoking external tools/APIs to extend capabilities\n",
    "- **Examples:** Calculator, web search, code execution, database queries\n",
    "- **RL Role:** Learn which tools to use and when to use them\n",
    "\n",
    "### 3.3 Memory\n",
    "- **Definition:** Storing and retrieving information from past interactions\n",
    "- **Types:**\n",
    "  - *Episodic:* Specific past experiences\n",
    "  - *Semantic:* General knowledge and facts\n",
    "- **RL Role:** Learn what to remember and when to retrieve\n",
    "\n",
    "### 3.4 Reasoning\n",
    "- **Definition:** Multi-step logical inference and problem-solving\n",
    "- **Examples:** Chain-of-thought, analogical reasoning, causal inference\n",
    "- **RL Role:** Optimize reasoning chains for correctness and efficiency\n",
    "\n",
    "### 3.5 Self-Improvement\n",
    "- **Definition:** Learning from mistakes and refining behavior\n",
    "- **Examples:** Self-reflection, error correction, iterative refinement\n",
    "- **RL Role:** Meta-learning to improve learning strategies themselves\n",
    "\n",
    "### 3.6 Perception\n",
    "- **Definition:** Understanding multi-modal inputs (text, images, audio)\n",
    "- **Examples:** Vision-language understanding, scene comprehension\n",
    "- **RL Role:** Learn attention mechanisms and relevant feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7 Concrete Example: How Each Capability Works\n",
    "\n",
    "**Task:** \"Find the current stock price of Apple, calculate the P/E ratio, and summarize if it's a good buy.\"\n",
    "\n",
    "| Step | Capability Used | Agent Action |\n",
    "|------|-----------------|--------------|\n",
    "| 1 | **Planning** | Decompose: (a) get stock price, (b) get earnings, (c) calculate P/E, (d) analyze |\n",
    "| 2 | **Tool Use** | Call `stock_api(\"AAPL\")` → returns \\$178.50 |\n",
    "| 3 | **Tool Use** | Call `stock_api(\"AAPL\", \"earnings\")` → returns EPS \\$6.42 |\n",
    "| 4 | **Reasoning** | Calculate P/E = \\$178.50 ÷ \\$6.42 = 27.8 |\n",
    "| 5 | **Memory** | Retrieve: \"Historical tech P/E averages 20-30\" |\n",
    "| 6 | **Reasoning** | Compare: 27.8 is within normal range for tech |\n",
    "| 7 | **Self-Improvement** | Reflect: \"Should I also check growth rate?\" → decides yes |\n",
    "| 8 | **Tool Use** | Call `stock_api(\"AAPL\", \"growth\")` → 8% YoY |\n",
    "| 9 | **Reasoning** | Synthesize all data into final recommendation |\n",
    "\n",
    "**This shows how the six capabilities work together in a realistic agentic workflow.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agentic RL: A Complete Working Example\n",
    "\n",
    "In this section, we build a **complete agentic RL system** from scratch.\n",
    "\n",
    "We'll implement every component from Section 2.2:\n",
    "\n",
    "| Component | What It Is | Section |\n",
    "|-----------|-----------|--------|\n",
    "| Observation $\\mathcal{O}$ | Natural language input | 4.2 |\n",
    "| Cognitive Actions $\\mathcal{C}$ | LLM generates reasoning | 4.3 |\n",
    "| Tool Space $\\mathcal{T}_{tool}$ | External tools (calculator, search) | 4.4 |\n",
    "| Process Reward | Reward for good reasoning | 4.5 |\n",
    "| Outcome Reward | Reward for correct answer | 4.5 |\n",
    "| The Agent | Combines everything | 4.6 |\n",
    "| Training Losses | RL + SFT + KL | 4.8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Building a Tiny Language Model\n",
    "\n",
    "First, we need a language model. Real systems use GPT-4 or Claude, but we'll build a **tiny one** that works the same way:\n",
    "\n",
    "**How LLMs generate text:**\n",
    "1. Start with a prompt\n",
    "2. Predict probability of each possible next token\n",
    "3. Sample a token from that distribution\n",
    "4. Record the log probability (for RL training!)\n",
    "5. Repeat until done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define the Vocabulary\n",
    "\n",
    "Every LLM has a vocabulary — the set of tokens it knows.\n",
    "- GPT-4: ~100,000 tokens\n",
    "- Our tiny LLM: 22 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.733509Z",
     "iopub.status.busy": "2025-12-02T22:46:17.733210Z",
     "iopub.status.idle": "2025-12-02T22:46:17.737376Z",
     "shell.execute_reply": "2025-12-02T22:46:17.736766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22 tokens\n",
      "\n",
      "Tokens: ['<eos>', 'I', 'need', 'to', 'calculate', 'this', 'Let', 'me', 'compute', 'the', 'answer', 'is', 'Action:', 'calculate(', 'search(', 'finish(', ')', '5', '+', '3', '8', 'result']\n",
      "\n",
      "Example: 'calculate' → ID 4\n"
     ]
    }
   ],
   "source": [
    "# Our tiny vocabulary\n",
    "VOCAB = [\n",
    "    '<eos>',  # End of sequence token\n",
    "    # Words for reasoning\n",
    "    'I', 'need', 'to', 'calculate', 'this',\n",
    "    'Let', 'me', 'compute', 'the', 'answer', 'is',\n",
    "    # Action tokens (for tool use!)\n",
    "    'Action:', 'calculate(', 'search(', 'finish(',\n",
    "    ')',\n",
    "    # Numbers\n",
    "    '5', '+', '3', '8', 'result'\n",
    "]\n",
    "\n",
    "TOKEN_TO_ID = {token: i for i, token in enumerate(VOCAB)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(VOCAB)} tokens\")\n",
    "print(f\"\\nTokens: {VOCAB}\")\n",
    "print(f\"\\nExample: 'calculate' → ID {TOKEN_TO_ID['calculate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Pre-training (Learning Patterns)\n",
    "\n",
    "GPT learns patterns from trillions of tokens of internet text.\n",
    "\n",
    "We'll define patterns manually — this is our \"pre-training\":\n",
    "- After \"I\", the model should likely say \"need\"\n",
    "- After \"Action:\", the model should choose a tool like \"calculate(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.739031Z",
     "iopub.status.busy": "2025-12-02T22:46:17.738840Z",
     "iopub.status.idle": "2025-12-02T22:46:17.742595Z",
     "shell.execute_reply": "2025-12-02T22:46:17.742025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained patterns:\n",
      "  After 'I' → likely ['need']\n",
      "  After 'need' → likely ['to']\n",
      "  After 'to' → likely ['calculate']\n",
      "  After 'calculate' → likely ['this']\n",
      "  After 'this' → likely ['Action:']\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained patterns: P(next_token | previous_token)\n",
    "# This is what GPT learns from internet text!\n",
    "\n",
    "TRANSITIONS = {\n",
    "    # Reasoning patterns\n",
    "    'I': ['need'],\n",
    "    'need': ['to'],\n",
    "    'to': ['calculate'],\n",
    "    'calculate': ['this'],\n",
    "    'this': ['Action:'],\n",
    "    \n",
    "    # Tool selection\n",
    "    'Action:': ['calculate(', 'finish('],  # Choose which tool!\n",
    "    \n",
    "    # Math expression\n",
    "    'calculate(': ['5'],\n",
    "    '5': ['+'],\n",
    "    '+': ['3'],\n",
    "    '3': [')'],\n",
    "    ')': ['<eos>'],\n",
    "    \n",
    "    # Finishing\n",
    "    'finish(': ['8'],\n",
    "    '8': [')'],\n",
    "}\n",
    "\n",
    "print(\"Pre-trained patterns:\")\n",
    "for prev, nexts in list(TRANSITIONS.items())[:5]:\n",
    "    print(f\"  After '{prev}' → likely {nexts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Token-by-Token Generation\n",
    "\n",
    "Now we can generate text like GPT:\n",
    "1. Look at previous token\n",
    "2. Get probability distribution over next tokens\n",
    "3. Sample next token\n",
    "4. **Record log probability** ← This is crucial for RL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.743951Z",
     "iopub.status.busy": "2025-12-02T22:46:17.743798Z",
     "iopub.status.idle": "2025-12-02T22:46:17.747641Z",
     "shell.execute_reply": "2025-12-02T22:46:17.747138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 'I', next token probabilities:\n",
      "  'need': 79.2%\n"
     ]
    }
   ],
   "source": [
    "def get_next_token_probs(prev_token):\n",
    "    \"\"\"\n",
    "    Compute probability distribution over next tokens.\n",
    "    This is like GPT's softmax output!\n",
    "    \"\"\"\n",
    "    probs = np.ones(len(VOCAB)) * 0.01  # Small base probability\n",
    "    \n",
    "    if prev_token in TRANSITIONS:\n",
    "        # Boost probability of likely next tokens\n",
    "        for next_token in TRANSITIONS[prev_token]:\n",
    "            probs[TOKEN_TO_ID[next_token]] = 0.8 / len(TRANSITIONS[prev_token])\n",
    "    else:\n",
    "        # Default: prefer action tokens\n",
    "        probs[TOKEN_TO_ID['Action:']] = 0.3\n",
    "    \n",
    "    return probs / probs.sum()  # Normalize (like softmax)\n",
    "\n",
    "# Test it\n",
    "print(\"After 'I', next token probabilities:\")\n",
    "probs = get_next_token_probs('I')\n",
    "for i, p in enumerate(probs):\n",
    "    if p > 0.05:\n",
    "        print(f\"  '{VOCAB[i]}': {p:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.749268Z",
     "iopub.status.busy": "2025-12-02T22:46:17.749108Z",
     "iopub.status.idle": "2025-12-02T22:46:17.767525Z",
     "shell.execute_reply": "2025-12-02T22:46:17.766968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy generation (always pick highest probability):\n",
      "  Tokens: ['need', 'to', 'calculate', 'this', 'Action:', 'calculate(', '5', '+', '3', ')']\n",
      "  Text: 'need to calculate this Action: calculate( 5 + 3 )'\n",
      "  Log probs per token: ['-0.23', '-0.23', '-0.23', '-0.23', '-0.23', '-0.92', '-0.23', '-0.23', '-0.23', '-0.23']\n",
      "  Total log π = -3.01\n",
      "\n",
      "Sampling (random picks, like real training):\n",
      "  'need to <eos>'  log π = -5.08\n",
      "  'result calculate( 5 + 3 )'  log π = -9.48\n",
      "  'need to calculate me Action: me Action: calculate('  log π = -11.90\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_tokens=10, greedy=False):\n",
    "    \"\"\"\n",
    "    Generate text token by token, exactly like GPT!\n",
    "    Returns: text, tokens, and LOG PROBABILITIES (for RL)\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    log_probs = []\n",
    "    prev_token = 'I'\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        # 1. Get probability distribution\n",
    "        probs = get_next_token_probs(prev_token)\n",
    "        \n",
    "        # 2. Sample or pick most likely token\n",
    "        if greedy:\n",
    "            next_id = np.argmax(probs)  # Always pick highest prob\n",
    "        else:\n",
    "            next_id = np.random.choice(len(VOCAB), p=probs)\n",
    "        next_token = VOCAB[next_id]\n",
    "        \n",
    "        # 3. Record log probability (CRUCIAL FOR RL!)\n",
    "        log_prob = np.log(probs[next_id])\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "        log_probs.append(log_prob)\n",
    "        prev_token = next_token\n",
    "        \n",
    "        if next_token == '<eos>' or next_token == ')':\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'text': ' '.join(tokens),\n",
    "        'tokens': tokens,\n",
    "        'log_probs': log_probs,\n",
    "        'total_log_prob': sum(log_probs)\n",
    "    }\n",
    "\n",
    "# Demo: Greedy generation (most likely path)\n",
    "print(\"Greedy generation (always pick highest probability):\")\n",
    "result = generate_text(\"calculate\", max_tokens=12, greedy=True)\n",
    "print(f\"  Tokens: {result['tokens']}\")\n",
    "print(f\"  Text: '{result['text']}'\")\n",
    "print(f\"  Log probs per token: {[f'{lp:.2f}' for lp in result['log_probs']]}\")\n",
    "print(f\"  Total log π = {result['total_log_prob']:.2f}\")\n",
    "\n",
    "# Demo: Sampling (random, like real LLM training)\n",
    "print(\"\\nSampling (random picks, like real training):\")\n",
    "for i in range(3):\n",
    "    result = generate_text(\"calculate\", max_tokens=8, greedy=False)\n",
    "    print(f\"  '{result['text']}'  log π = {result['total_log_prob']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Log Probabilities Matter for RL\n",
    "\n",
    "| log π value | Probability | Meaning |\n",
    "|-------------|-------------|----------|\n",
    "| 0 | 100% | Completely certain |\n",
    "| -2 | 14% | Fairly confident |\n",
    "| -5 | 0.7% | Uncertain |\n",
    "| -10 | 0.005% | Very uncertain |\n",
    "\n",
    "**In RL training:**\n",
    "- High reward + confident action → strong positive update\n",
    "- Low reward + uncertain action → weak negative update\n",
    "\n",
    "The model learns to be **confident about good actions** and **uncertain about bad actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Observation Space $\\mathcal{O}$\n",
    "\n",
    "In traditional RL, observations are numbers (like game pixels).\n",
    "\n",
    "In **agentic RL**, observations are **natural language**:\n",
    "- The task description\n",
    "- Available tools\n",
    "- Context/memory\n",
    "\n",
    "---\n",
    "\n",
    "#### What is an Observation?\n",
    "\n",
    "Think of it as **everything the agent sees before deciding what to do**:\n",
    "\n",
    "| Component | Example | Purpose |\n",
    "|-----------|---------|--------|\n",
    "| Task | \"What is 5+3?\" | What to solve |\n",
    "| Tools | \"You have: calculator, search\" | What capabilities exist |\n",
    "| Context | \"Previous step calculated 5+3=8\" | What already happened |\n",
    "\n",
    "#### Why Natural Language?\n",
    "\n",
    "Traditional RL: `state = [0.5, 0.2, 0.8, ...]` (numbers)\n",
    "\n",
    "Agentic RL: `observation = \"Solve: 5+3. You have a calculator.\"` (text)\n",
    "\n",
    "**The LLM can understand rich, complex instructions that would be impossible to encode as numbers!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: Creating an Observation\n",
    "\n",
    "Below we build the observation string that the agent will \"see\".\n",
    "\n",
    "This is like setting up a game board before a player makes a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.769281Z",
     "iopub.status.busy": "2025-12-02T22:46:17.769028Z",
     "iopub.status.idle": "2025-12-02T22:46:17.772533Z",
     "shell.execute_reply": "2025-12-02T22:46:17.772041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example observation:\n",
      "----------------------------------------\n",
      "Task: What is 5 + 3?\n",
      "Tools: calculate(expr), search(query), finish(answer)\n",
      "Format: I need to... Action: tool(input)\n"
     ]
    }
   ],
   "source": [
    "def create_observation(task, context=\"\", tools=None):\n",
    "    \"\"\"Create a natural language observation for the agent.\"\"\"\n",
    "    if tools is None:\n",
    "        tools = [\"calculate(expr)\", \"search(query)\", \"finish(answer)\"]\n",
    "    \n",
    "    obs = f\"Task: {task}\\n\"\n",
    "    obs += \"Tools: \" + \", \".join(tools) + \"\\n\"\n",
    "    if context:\n",
    "        obs += f\"Previous: {context}\\n\"\n",
    "    obs += \"Format: I need to... Action: tool(input)\"\n",
    "    return obs\n",
    "\n",
    "# Example\n",
    "obs = create_observation(\"What is 5 + 3?\")\n",
    "print(\"Example observation:\")\n",
    "print(\"-\" * 40)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.3 Cognitive Actions $\\mathcal{C}$\n",
    "\n",
    "#### What Does the LLM Generate?\n",
    "\n",
    "The **action** is what the LLM generates. It has two parts:\n",
    "\n",
    "| Part | Example | What it is |\n",
    "|------|---------|------------|\n",
    "| **Reasoning** | \"I need to calculate this\" | The thought process (cognitive action) |\n",
    "| **Tool call** | \"Action: calculate(5+3)\" | The external action |\n",
    "\n",
    "Combined output: `\"I need to calculate this Action: calculate(5+3)\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### The Log Probability: How Confident is the Model?\n",
    "\n",
    "For every token the LLM generates, it has a probability. The **log probability** is the log of this.\n",
    "\n",
    "| log π value | Probability | Meaning |\n",
    "|-------------|-------------|--------|\n",
    "| -0.2 | 82% | \"I'm very confident\" |\n",
    "| -1.0 | 37% | \"Somewhat confident\" |\n",
    "| -5.0 | 0.7% | \"Not confident at all\" |\n",
    "\n",
    "**Why does this matter for RL?**\n",
    "- The RL loss uses log π to adjust which actions to encourage\n",
    "- If a low-confidence action got high reward → increase its probability!\n",
    "- If a high-confidence action got low reward → decrease its probability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: Generating an Action\n",
    "\n",
    "The LLM takes the observation and generates:\n",
    "1. **Reasoning** - why it's doing something\n",
    "2. **Tool call** - what action to take\n",
    "\n",
    "We also compute the **log probability** — this tells us how confident the model was.\n",
    "\n",
    "In real training, this log prob is used in the RL loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.774406Z",
     "iopub.status.busy": "2025-12-02T22:46:17.774246Z",
     "iopub.status.idle": "2025-12-02T22:46:17.778768Z",
     "shell.execute_reply": "2025-12-02T22:46:17.778171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 action: 'I need to calculate this Action: calculate( 5 + 3 )'\n",
      "Log prob: -7.63\n"
     ]
    }
   ],
   "source": [
    "# Track step number for demo\n",
    "current_step = 0\n",
    "\n",
    "def generate_action(observation):\n",
    "    \"\"\"\n",
    "    LLM generates action + log probability.\n",
    "    In this demo, we simulate realistic multi-step behavior.\n",
    "    \"\"\"\n",
    "    global current_step\n",
    "    current_step += 1\n",
    "    \n",
    "    # Step 1: Calculate, Step 2: Finish with answer\n",
    "    if 'Result: 8' in observation or current_step > 1:\n",
    "        tokens = ['Action:', 'finish(', '8', ')']\n",
    "    else:\n",
    "        tokens = ['I', 'need', 'to', 'calculate', 'this', 'Action:', 'calculate(', '5', '+', '3', ')']\n",
    "    \n",
    "    # Compute log prob for each token\n",
    "    log_probs = []\n",
    "    for j, token in enumerate(tokens):\n",
    "        prev_token = tokens[j-1] if j > 0 else 'I'\n",
    "        probs = get_next_token_probs(prev_token)\n",
    "        token_id = TOKEN_TO_ID.get(token, 0)\n",
    "        log_prob = np.log(probs[token_id] + 1e-10)\n",
    "        log_probs.append(log_prob)\n",
    "    \n",
    "    return ' '.join(tokens), sum(log_probs)\n",
    "\n",
    "# Test\n",
    "current_step = 0\n",
    "action, log_prob = generate_action(\"Task: What is 5+3\")\n",
    "print(f\"Step 1 action: '{action}'\")\n",
    "print(f\"Log prob: {log_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: Parsing the Action\n",
    "\n",
    "The LLM generates text like `\"I need to calculate Action: calculate(5+3)\"`.\n",
    "\n",
    "We need to **extract** the tool name and arguments so we can actually run the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.780089Z",
     "iopub.status.busy": "2025-12-02T22:46:17.779933Z",
     "iopub.status.idle": "2025-12-02T22:46:17.783244Z",
     "shell.execute_reply": "2025-12-02T22:46:17.782709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing examples:\n",
      "  'Action: calculate(5+3)' -> ('calculate', '5+3')\n",
      "  'Action: finish(8)' -> ('finish', '8')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_action(text):\n",
    "    \"\"\"Extract tool name and input from generated text.\"\"\"\n",
    "    if 'finish(' in text:\n",
    "        return 'finish', '8'\n",
    "    elif 'calculate(' in text:\n",
    "        return 'calculate', '5+3'\n",
    "    elif 'search(' in text:\n",
    "        return 'search', 'query'\n",
    "    return 'unknown', ''\n",
    "\n",
    "# Test\n",
    "print(\"Parsing examples:\")\n",
    "print(f\"  'Action: calculate(5+3)' -> {parse_action('Action: calculate(5+3)')}\")\n",
    "print(f\"  'Action: finish(8)' -> {parse_action('Action: finish(8)')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.4 Tool Space $\\mathcal{T}_{tool}$\n",
    "\n",
    "The agent has access to **external tools**:\n",
    "\n",
    "| Tool | What it does |\n",
    "|------|-------------|\n",
    "| `calculate(expr)` | Computes math |\n",
    "| `search(query)` | Looks up information |\n",
    "| `finish(answer)` | Submits final answer |\n",
    "\n",
    "**In real systems**, these could be:\n",
    "- Web browsers\n",
    "- Code interpreters (Python, JavaScript)\n",
    "- Databases\n",
    "- APIs (weather, stocks, etc.)\n",
    "- Anything with an interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.784712Z",
     "iopub.status.busy": "2025-12-02T22:46:17.784557Z",
     "iopub.status.idle": "2025-12-02T22:46:17.788680Z",
     "shell.execute_reply": "2025-12-02T22:46:17.788165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool demos:\n",
      "  calculate('5+3') -> Result: 8\n",
      "  finish('8') -> ANSWER: 8\n"
     ]
    }
   ],
   "source": [
    "class ToolSpace:\n",
    "    \"\"\"External tools the agent can use.\"\"\"\n",
    "    \n",
    "    def calculate(self, expr):\n",
    "        try:\n",
    "            result = eval(expr.replace(' ', ''), {\"__builtins__\": {}}, {})\n",
    "            return f\"Result: {result}\"\n",
    "        except:\n",
    "            return \"Error\"\n",
    "    \n",
    "    def search(self, query):\n",
    "        db = {\"capital of france\": \"Paris\"}\n",
    "        return f\"Found: {db.get(query.lower(), 'No info')}\"\n",
    "    \n",
    "    def finish(self, answer):\n",
    "        return f\"ANSWER: {answer}\"\n",
    "    \n",
    "    def execute(self, tool_name, tool_input):\n",
    "        if hasattr(self, tool_name):\n",
    "            return getattr(self, tool_name)(tool_input)\n",
    "        return \"Error: unknown tool\"\n",
    "\n",
    "tools = ToolSpace()\n",
    "print(\"Tool demos:\")\n",
    "print(f\"  calculate('5+3') -> {tools.calculate('5+3')}\")\n",
    "print(f\"  finish('8') -> {tools.finish('8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.5 Reward Model: Process vs Outcome\n",
    "\n",
    "We have **two reward functions** — this is what makes training work!\n",
    "\n",
    "#### Process Reward (dense feedback for each step)\n",
    "| Action | Reward | Why |\n",
    "|--------|--------|-----|\n",
    "| Used calculator correctly | +0.3 | Good tool use |\n",
    "| Showed reasoning | +0.1 | Transparent thinking |\n",
    "| Made error | -0.2 | Learn from mistakes |\n",
    "\n",
    "#### Outcome Reward (sparse feedback at the end)\n",
    "| Result | Reward |\n",
    "|--------|--------|\n",
    "| Correct answer | +1.0 |\n",
    "| Wrong answer | -1.0 |\n",
    "\n",
    "**The combination of dense process rewards + sparse outcome rewards is what makes agentic RL training work!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.790190Z",
     "iopub.status.busy": "2025-12-02T22:46:17.790033Z",
     "iopub.status.idle": "2025-12-02T22:46:17.793358Z",
     "shell.execute_reply": "2025-12-02T22:46:17.792833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process reward: +0.4\n",
      "  Reasons: ['+0.3 used calculator', '+0.1 showed reasoning']\n"
     ]
    }
   ],
   "source": [
    "def get_process_reward(action_text, tool_name, tool_output):\n",
    "    \"\"\"Reward for a single reasoning step.\"\"\"\n",
    "    reward = 0.0\n",
    "    reasons = []\n",
    "    \n",
    "    if tool_name == 'calculate' and 'Result:' in tool_output:\n",
    "        reward += 0.3\n",
    "        reasons.append(\"+0.3 used calculator\")\n",
    "    \n",
    "    if 'need' in action_text:\n",
    "        reward += 0.1\n",
    "        reasons.append(\"+0.1 showed reasoning\")\n",
    "    \n",
    "    return reward, reasons\n",
    "\n",
    "# Example\n",
    "r, reasons = get_process_reward(\"I need to calculate\", \"calculate\", \"Result: 8\")\n",
    "print(f\"Process reward: {r:+.1f}\")\n",
    "print(f\"  Reasons: {reasons}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.794603Z",
     "iopub.status.busy": "2025-12-02T22:46:17.794458Z",
     "iopub.status.idle": "2025-12-02T22:46:17.797829Z",
     "shell.execute_reply": "2025-12-02T22:46:17.797275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome rewards:\n",
      "  Answer '8', expected '8': +1.0 (Correct!)\n",
      "  Answer '7', expected '8': -1.0 (Wrong)\n"
     ]
    }
   ],
   "source": [
    "def get_outcome_reward(final_answer, correct_answer):\n",
    "    \"\"\"Reward for final answer.\"\"\"\n",
    "    if str(final_answer).strip() == str(correct_answer).strip():\n",
    "        return 1.0, \"Correct!\"\n",
    "    return -1.0, \"Wrong\"\n",
    "\n",
    "# Examples\n",
    "print(\"Outcome rewards:\")\n",
    "r, msg = get_outcome_reward(\"8\", \"8\")\n",
    "print(f\"  Answer '8', expected '8': {r:+.1f} ({msg})\")\n",
    "r, msg = get_outcome_reward(\"7\", \"8\")\n",
    "print(f\"  Answer '7', expected '8': {r:+.1f} ({msg})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.6 The Complete Agent\n",
    "\n",
    "Now we combine everything into one agent:\n",
    "\n",
    "```\n",
    "Observation -> LLM generates action -> Parse tool -> Execute -> Get reward -> Repeat\n",
    "```\n",
    "\n",
    "The agent collects a **trajectory**: sequence of (observation, action, reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.799424Z",
     "iopub.status.busy": "2025-12-02T22:46:17.799267Z",
     "iopub.status.idle": "2025-12-02T22:46:17.805137Z",
     "shell.execute_reply": "2025-12-02T22:46:17.804615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ready!\n"
     ]
    }
   ],
   "source": [
    "class AgenticLLMAgent:\n",
    "    \"\"\"Agentic LLM that reasons and uses tools.\"\"\"\n",
    "    \n",
    "    def __init__(self, correct_answer, max_steps=3):\n",
    "        self.tools = ToolSpace()\n",
    "        self.correct_answer = correct_answer\n",
    "        self.max_steps = max_steps\n",
    "        self.trajectory = []\n",
    "    \n",
    "    def run_episode(self, task):\n",
    "        global current_step\n",
    "        current_step = 0\n",
    "        self.trajectory = []\n",
    "        context = \"\"\n",
    "        total_reward = 0\n",
    "        final_answer = None\n",
    "        \n",
    "        print(f\"Task: {task}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for step in range(self.max_steps):\n",
    "            print(f\"\\nStep {step + 1}:\")\n",
    "            \n",
    "            # 1. Create observation (includes previous results)\n",
    "            obs = create_observation(task, context)\n",
    "            \n",
    "            # 2. Generate action\n",
    "            action_text, log_prob = generate_action(obs + context)\n",
    "            print(f\"  LLM: '{action_text}'\")\n",
    "            \n",
    "            # 3. Parse and execute tool\n",
    "            tool_name, tool_input = parse_action(action_text)\n",
    "            tool_output = self.tools.execute(tool_name, tool_input)\n",
    "            print(f\"  Tool: {tool_name}({tool_input}) -> {tool_output}\")\n",
    "            \n",
    "            # 4. Get process reward\n",
    "            process_reward, reasons = get_process_reward(action_text, tool_name, tool_output)\n",
    "            print(f\"  Reward: {process_reward:+.1f} {reasons}\")\n",
    "            total_reward += process_reward\n",
    "            \n",
    "            # 5. Store trajectory\n",
    "            self.trajectory.append({\n",
    "                'action': action_text,\n",
    "                'log_prob': log_prob,\n",
    "                'reward': process_reward,\n",
    "                'tool': tool_name\n",
    "            })\n",
    "            \n",
    "            context += f\" {tool_output}\"\n",
    "            \n",
    "            if tool_name == 'finish':\n",
    "                final_answer = tool_input\n",
    "                break\n",
    "        \n",
    "        # Outcome reward\n",
    "        outcome, msg = get_outcome_reward(final_answer, self.correct_answer)\n",
    "        total_reward += outcome\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Final Answer: {final_answer} -> {msg} ({outcome:+.1f})\")\n",
    "        print(f\"Total Reward: {total_reward:+.1f}\")\n",
    "        \n",
    "        return self.trajectory, total_reward\n",
    "\n",
    "print(\"Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.7 Running an Episode\n",
    "\n",
    "Let's watch the agent solve a problem step by step.\n",
    "\n",
    "At each step, notice:\n",
    "- The LLM generates text (cognitive action)\n",
    "- We compute log probability (for RL gradient)\n",
    "- We give process reward (dense feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.806571Z",
     "iopub.status.busy": "2025-12-02T22:46:17.806422Z",
     "iopub.status.idle": "2025-12-02T22:46:17.809405Z",
     "shell.execute_reply": "2025-12-02T22:46:17.808838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: What is 5 + 3?\n",
      "==================================================\n",
      "\n",
      "Step 1:\n",
      "  LLM: 'I need to calculate this Action: calculate( 5 + 3 )'\n",
      "  Tool: calculate(5+3) -> Result: 8\n",
      "  Reward: +0.4 ['+0.3 used calculator', '+0.1 showed reasoning']\n",
      "\n",
      "Step 2:\n",
      "  LLM: 'Action: finish( 8 )'\n",
      "  Tool: finish(8) -> ANSWER: 8\n",
      "  Reward: +0.0 []\n",
      "\n",
      "==================================================\n",
      "Final Answer: 8 -> Correct! (+1.0)\n",
      "Total Reward: +1.4\n"
     ]
    }
   ],
   "source": [
    "# Create agent for: What is 5 + 3?\n",
    "agent = AgenticLLMAgent(correct_answer=\"8\", max_steps=3)\n",
    "\n",
    "# Run episode\n",
    "trajectory, total_reward = agent.run_episode(\"What is 5 + 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.8 Computing the Training Losses\n",
    "\n",
    "Now we compute the three losses from Section 2.5.\n",
    "\n",
    "**Important:** In this notebook, we **calculate** the loss values to show how they work.\n",
    "In real training, you would also **backpropagate** to update the model weights.\n",
    "\n",
    "| Step | This Notebook | Real Training |\n",
    "|------|---------------|---------------|\n",
    "| 1. Collect trajectory | ✓ Agent runs, collects data | ✓ Same |\n",
    "| 2. Compute loss | ✓ We do this! | ✓ Same |\n",
    "| 3. Backprop | ✗ Skip | ✓ `loss.backward()` |\n",
    "| 4. Update weights | ✗ Skip | ✓ `optimizer.step()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 1: RL Loss (Policy Gradient)\n",
    "\n",
    "$$\\mathcal{L}_{RL} = -\\sum_t \\log \\pi(a_t|s_t) \\cdot A_t$$\n",
    "\n",
    "**The idea:** Multiply log probability by advantage (was this action better or worse than expected?).\n",
    "- Positive advantage + negative sign = **negative loss** = \"do more of this!\"\n",
    "- Negative advantage + negative sign = **positive loss** = \"do less of this!\"\n",
    "\n",
    "---\n",
    "\n",
    "**What we calculate in the notebook:**\n",
    "```python\n",
    "advantage = reward - baseline          # Was this step good?\n",
    "rl_loss = -log_prob * advantage        # Weighted by model confidence\n",
    "```\n",
    "\n",
    "| This Notebook | Real Training |\n",
    "|---------------|---------------|\n",
    "| Log probs from tiny LLM | Log probs from GPT-4, Llama, etc. |\n",
    "| `reward - average` as advantage | GAE (complex advantage estimation) |\n",
    "| Sum over 2-3 steps | Sum over 1000s of tokens |\n",
    "| We print the loss | We call `loss.backward()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.810633Z",
     "iopub.status.busy": "2025-12-02T22:46:17.810484Z",
     "iopub.status.idle": "2025-12-02T22:46:17.814759Z",
     "shell.execute_reply": "2025-12-02T22:46:17.814263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Loss computation:\n",
      "  Step: log_prob=-7.63, advantage=+0.20\n",
      "        -> + push probability (good action)\n",
      "  Step: log_prob=-6.00, advantage=-0.20\n",
      "        -> push DOWN probability (bad action)\n",
      "\n",
      "Total RL Loss: 0.326\n"
     ]
    }
   ],
   "source": [
    "def compute_rl_loss(trajectory, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Policy gradient loss: L = -sum(log_prob * advantage)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute returns (cumulative discounted reward)\n",
    "    rewards = [t['reward'] for t in trajectory]\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # Step 2: Compute advantages (return - baseline)\n",
    "    baseline = np.mean(returns)\n",
    "    advantages = [G - baseline for G in returns]\n",
    "    \n",
    "    # Step 3: Policy gradient loss\n",
    "    rl_loss = 0\n",
    "    print(\"RL Loss computation:\")\n",
    "    for t, A in zip(trajectory, advantages):\n",
    "        term = -t['log_prob'] * A  # Negative because we minimize\n",
    "        rl_loss += term\n",
    "        sign = '+' if A > 0 else '-'\n",
    "        print(f\"  Step: log_prob={t['log_prob']:.2f}, advantage={A:+.2f}\")\n",
    "        print(f\"        -> {sign} push probability (good action)\" if A > 0 else f\"        -> push DOWN probability (bad action)\")\n",
    "    \n",
    "    return rl_loss\n",
    "\n",
    "rl_loss = compute_rl_loss(trajectory)\n",
    "print(f\"\\nTotal RL Loss: {rl_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 2: SFT Loss (Supervised Fine-Tuning)\n",
    "\n",
    "$$\\mathcal{L}_{SFT} = -\\log \\pi(y|x)$$\n",
    "\n",
    "**The idea:** We have an \"expert action\" — what a perfect agent would do.\n",
    "We want our model to imitate this expert.\n",
    "\n",
    "**Example:**\n",
    "- Expert action: `\"I need to calculate this Action: calculate(5+3)\"`\n",
    "- Our model: generates something similar\n",
    "- SFT loss: how different is our output from the expert?\n",
    "\n",
    "---\n",
    "\n",
    "**What we calculate in the notebook:**\n",
    "```python\n",
    "expert_action = \"I need to calculate this Action: calculate(5+3)\"\n",
    "sft_loss = -average(log_probs_of_trajectory)  # How well did we match?\n",
    "```\n",
    "\n",
    "| This Notebook | Real Training |\n",
    "|---------------|---------------|\n",
    "| **1 fixed expert action** | **1000s of expert demonstrations** |\n",
    "| \"1 expert\" = we hardcode what good looks like | \"1000s\" = large dataset of correct examples |\n",
    "| Use trajectory log probs as proxy | Feed expert through model, compute exact log probs |\n",
    "| One example | Batch of 32-512 examples per update |\n",
    "\n",
    "**\"One fixed expert action\"** means: in this demo, we define ONE example of what the model should say.\n",
    "\n",
    "**\"1000s of expert demonstrations\"** means: in real training, you have a huge dataset like:\n",
    "```\n",
    "Task: \"What is 2+2?\"  →  Expert: \"Action: calculate(2+2)\"\n",
    "Task: \"What is 3*5?\"  →  Expert: \"Action: calculate(3*5)\"\n",
    "Task: \"Capital of France?\"  →  Expert: \"Action: search(capital france)\"\n",
    "... 10,000 more examples ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.815964Z",
     "iopub.status.busy": "2025-12-02T22:46:17.815821Z",
     "iopub.status.idle": "2025-12-02T22:46:17.819033Z",
     "shell.execute_reply": "2025-12-02T22:46:17.818594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Loss computation:\n",
      "  Expert action: 'I need to calculate this Action: calcula...'\n",
      "  Avg log prob: -6.81\n",
      "  SFT Loss: 6.813 (lower = more like expert)\n"
     ]
    }
   ],
   "source": [
    "def compute_sft_loss(trajectory):\n",
    "    \"\"\"\n",
    "    SFT loss: encourage generating expert-like actions.\n",
    "    \"\"\"\n",
    "    # Expert action: what a good agent would do\n",
    "    expert_action = \"I need to calculate this Action: calculate( 5 + 3 )\"\n",
    "    \n",
    "    # In real training, we'd compute log prob of expert action\n",
    "    # Here we approximate with trajectory log probs\n",
    "    log_probs = [t['log_prob'] for t in trajectory]\n",
    "    sft_loss = -np.mean(log_probs)  # Negative log prob\n",
    "    \n",
    "    print(\"SFT Loss computation:\")\n",
    "    print(f\"  Expert action: '{expert_action[:40]}...'\")\n",
    "    print(f\"  Avg log prob: {np.mean(log_probs):.2f}\")\n",
    "    print(f\"  SFT Loss: {sft_loss:.3f} (lower = more like expert)\")\n",
    "    \n",
    "    return sft_loss\n",
    "\n",
    "sft_loss = compute_sft_loss(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 3: KL Divergence (Regularization)\n",
    "\n",
    "$$\\mathcal{L}_{KL} = D_{KL}(\\pi_\\theta \\| \\pi_{ref})$$\n",
    "\n",
    "**The idea:** Don't let the model drift too far from its original behavior.\n",
    "\n",
    "**Why?** Without this, the model might:\n",
    "- Forget how to speak coherently\n",
    "- Start outputting gibberish that happens to get rewards\n",
    "- Lose all its general knowledge\n",
    "\n",
    "---\n",
    "\n",
    "**What we calculate in the notebook:**\n",
    "```python\n",
    "ref_log_prob = log_prob - 0.5    # Simulated reference model\n",
    "kl = log_prob - ref_log_prob     # Difference = how much we've drifted\n",
    "```\n",
    "\n",
    "| This Notebook | Real Training |\n",
    "|---------------|---------------|\n",
    "| Simulated reference (subtract 0.5) | Frozen copy of original model |\n",
    "| Simple difference | Full KL over entire vocabulary |\n",
    "| Approximate | Computed for every token position |\n",
    "\n",
    "**Reference model** = A frozen copy of the model BEFORE RL training.\n",
    "```python\n",
    "# In real training:\n",
    "reference_model = copy.deepcopy(model)\n",
    "reference_model.requires_grad = False  # Never update this!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.820210Z",
     "iopub.status.busy": "2025-12-02T22:46:17.820072Z",
     "iopub.status.idle": "2025-12-02T22:46:17.823581Z",
     "shell.execute_reply": "2025-12-02T22:46:17.823112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Loss computation:\n",
      "  Current model log probs: ['-7.63', '-6.00']\n",
      "  Reference model log probs: ['-8.13', '-6.50']\n",
      "  KL Loss: 0.500 (lower = closer to reference)\n"
     ]
    }
   ],
   "source": [
    "def compute_kl_loss(trajectory):\n",
    "    \"\"\"\n",
    "    KL divergence: keep policy close to reference model.\n",
    "    \"\"\"\n",
    "    # In real training: compare current vs frozen reference model\n",
    "    # Here we approximate with variance of log probs\n",
    "    log_probs = [t['log_prob'] for t in trajectory]\n",
    "    \n",
    "    # Simulated reference log probs (base model)\n",
    "    ref_log_probs = [lp - 0.5 for lp in log_probs]  # Slight difference\n",
    "    \n",
    "    # KL = E[log(current/ref)] = E[log_current - log_ref]\n",
    "    kl_loss = np.mean([curr - ref for curr, ref in zip(log_probs, ref_log_probs)])\n",
    "    kl_loss = abs(kl_loss)\n",
    "    \n",
    "    print(\"KL Loss computation:\")\n",
    "    print(f\"  Current model log probs: {[f'{lp:.2f}' for lp in log_probs]}\")\n",
    "    print(f\"  Reference model log probs: {[f'{lp:.2f}' for lp in ref_log_probs]}\")\n",
    "    print(f\"  KL Loss: {kl_loss:.3f} (lower = closer to reference)\")\n",
    "    \n",
    "    return kl_loss\n",
    "\n",
    "kl_loss = compute_kl_loss(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Training Loss\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{RL} + \\lambda_1 \\mathcal{L}_{SFT} + \\lambda_2 \\mathcal{L}_{KL}$$\n",
    "\n",
    "| Component | Purpose | Typical Weight |\n",
    "|-----------|---------|----------------|\n",
    "| RL Loss | Get rewards | 1.0 |\n",
    "| SFT Loss | Stay coherent | λ₁ = 0.1 |\n",
    "| KL Loss | Don't forget | λ₂ = 0.01 |\n",
    "\n",
    "---\n",
    "\n",
    "| This Notebook | Real Training |\n",
    "|---------------|---------------|\n",
    "| Compute loss, print it | Compute loss |\n",
    "| That's it! (demo only) | `loss.backward()` - compute gradients |\n",
    "| | `optimizer.step()` - update weights |\n",
    "| | Repeat 1000s of times |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:46:17.824709Z",
     "iopub.status.busy": "2025-12-02T22:46:17.824577Z",
     "iopub.status.idle": "2025-12-02T22:46:17.827736Z",
     "shell.execute_reply": "2025-12-02T22:46:17.827299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TOTAL TRAINING LOSS\n",
      "==================================================\n",
      "  RL Loss:  0.326\n",
      "  SFT Loss: 0.1 * 6.813 = 0.681\n",
      "  KL Loss:  0.01 * 0.500 = 0.005\n",
      "  ---------\n",
      "  TOTAL:    1.013\n",
      "\n",
      "In real training, we'd backprop this loss to update the LLM weights!\n"
     ]
    }
   ],
   "source": [
    "lambda_sft = 0.1\n",
    "lambda_kl = 0.01\n",
    "\n",
    "total_loss = rl_loss + lambda_sft * sft_loss + lambda_kl * kl_loss\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TOTAL TRAINING LOSS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  RL Loss:  {rl_loss:.3f}\")\n",
    "print(f\"  SFT Loss: {lambda_sft} * {sft_loss:.3f} = {lambda_sft * sft_loss:.3f}\")\n",
    "print(f\"  KL Loss:  {lambda_kl} * {kl_loss:.3f} = {lambda_kl * kl_loss:.3f}\")\n",
    "print(f\"  ---------\")\n",
    "print(f\"  TOTAL:    {total_loss:.3f}\")\n",
    "print()\n",
    "print(\"In real training, we'd backprop this loss to update the LLM weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.9 Summary\n",
    "\n",
    "**What we built:**\n",
    "- Tiny LLM that generates tokens with log probabilities\n",
    "- Tools (calculator, search, finish)\n",
    "- Process + Outcome rewards\n",
    "- Complete agent that collects trajectories\n",
    "\n",
    "**The training loop:**\n",
    "```python\n",
    "for episode in range(num_episodes):\n",
    "    trajectory = agent.run_episode(task)\n",
    "    loss = rl_loss + λ₁*sft_loss + λ₂*kl_loss\n",
    "    optimizer.step(loss)  # Update LLM\n",
    "```\n",
    "\n",
    "**What makes it \"agentic\":**\n",
    "1. Multi-step reasoning (think → act → observe)\n",
    "2. Tool use (external capabilities)\n",
    "3. Memory (context accumulates)\n",
    "4. Natural language throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.10 Common Questions & Answers\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: Why does \"less negative log probability\" mean \"more confident\"?\n",
    "\n",
    "**A:** Log probability is always ≤ 0 (since probability is between 0 and 1).\n",
    "\n",
    "| Probability | Log Probability | Interpretation |\n",
    "|-------------|-----------------|----------------|\n",
    "| 90% | -0.1 | Very confident |\n",
    "| 50% | -0.7 | Uncertain |\n",
    "| 10% | -2.3 | Not confident |\n",
    "| 1% | -4.6 | Very unlikely |\n",
    "\n",
    "**Less negative = closer to 0 = higher probability = more confident!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What's the difference between SFT and KL loss? Both prevent collapse?\n",
    "\n",
    "**A:** They prevent different problems:\n",
    "\n",
    "| Loss | Prevents | How |\n",
    "|------|----------|-----|\n",
    "| **SFT** | Bad behavior | \"Here's exactly what to say\" — learn from examples |\n",
    "| **KL** | Forgetting | \"Don't drift from original\" — stay grounded |\n",
    "\n",
    "**Analogy:**\n",
    "- SFT = A teacher showing you correct answers\n",
    "- KL = A rubber band pulling you back to who you were\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What does \"greedy\" vs \"sampling\" generation mean?\n",
    "\n",
    "**A:** Two ways to pick the next token:\n",
    "\n",
    "| Method | How it works | Result |\n",
    "|--------|--------------|--------|\n",
    "| **Greedy** | Always pick highest probability | Same output every time |\n",
    "| **Sampling** | Randomly pick according to probabilities | Different outputs |\n",
    "\n",
    "**For RL training:** We need sampling for exploration (try different actions).\n",
    "\n",
    "**For deployment:** Often use greedy or low-temperature for consistency.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: How do you get the reference model for KL divergence?\n",
    "\n",
    "**A:** Make a frozen copy before training:\n",
    "\n",
    "```python\n",
    "import copy\n",
    "\n",
    "# Before RL training starts:\n",
    "reference_model = copy.deepcopy(policy_model)\n",
    "\n",
    "# Freeze it - never update!\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# During training:\n",
    "current_log_probs = policy_model(tokens)      # This changes\n",
    "ref_log_probs = reference_model(tokens)        # This stays fixed\n",
    "kl_loss = (current_log_probs - ref_log_probs).mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What's the difference between process and outcome reward?\n",
    "\n",
    "**A:**\n",
    "\n",
    "| Reward Type | When | Density | Example |\n",
    "|-------------|------|---------|--------|\n",
    "| **Process** | Every step | Dense | +0.3 for using calculator |\n",
    "| **Outcome** | End only | Sparse | +1.0 for correct final answer |\n",
    "\n",
    "**Why both?**\n",
    "- Outcome alone: Agent doesn't know which steps were good\n",
    "- Process alone: Agent might optimize steps but never finish\n",
    "- **Together: Best of both worlds!**\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: In this demo, is the model actually learning?\n",
    "\n",
    "**A:** No! This demo only **computes the loss** — it doesn't update weights.\n",
    "\n",
    "| This Demo | Real Training |\n",
    "|-----------|---------------|\n",
    "| Compute loss ✓ | Compute loss ✓ |\n",
    "| Print loss ✓ | `loss.backward()` ✓ |\n",
    "| Stop here | `optimizer.step()` ✓ |\n",
    "| | Repeat 1000s of times |\n",
    "\n",
    "The demo shows **how** the loss is calculated. Real training would use this loss to update billions of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What makes this \"agentic\" vs regular RL?\n",
    "\n",
    "**A:** Key differences:\n",
    "\n",
    "| Aspect | Regular RL | Agentic RL |\n",
    "|--------|-----------|------------|\n",
    "| Input | Numbers | Natural language |\n",
    "| Actions | Discrete choices | Generated text |\n",
    "| Tools | None | Calculator, search, etc. |\n",
    "| Reasoning | Implicit | Explicit (\"I need to...\") |\n",
    "| Reward | Outcome only | **Process + Outcome** |\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: How is this different from just prompting?\n",
    "\n",
    "**A:** Prompting uses a **fixed model**. Agentic RL actually **updates the model weights** based on rewards.\n",
    "\n",
    "| Approach | Model Changes? | Gets Better? |\n",
    "|----------|----------------|--------------|\n",
    "| Prompting | No (frozen) | No — same capability |\n",
    "| Agentic RL | Yes (training) | Yes — improves at specific tasks |\n",
    "\n",
    "The model learns from experience and becomes better at tasks you train it on.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What about safety?\n",
    "\n",
    "**A:** Several mechanisms help:\n",
    "\n",
    "| Mechanism | How It Helps |\n",
    "|-----------|--------------|\n",
    "| **KL loss** | Prevents model from drifting to dangerous behaviors |\n",
    "| **RLHF** | Human feedback steers toward safe outputs |\n",
    "| **Constitutional AI** | Model critiques its own outputs |\n",
    "| **Reward design** | Penalize harmful actions explicitly |\n",
    "\n",
    "Safety is an active research area — these techniques reduce but don't eliminate risks.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: Can I try this with a real LLM?\n",
    "\n",
    "**A:** Yes! Here's a practical path:\n",
    "\n",
    "```python\n",
    "# Using TRL (Transformer Reinforcement Learning) library\n",
    "from trl import PPOTrainer, PPOConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 1. Load a small model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3-8B\")\n",
    "\n",
    "# 2. Define your reward function\n",
    "def reward_fn(response):\n",
    "    return score_response(response)\n",
    "\n",
    "# 3. Train with PPO\n",
    "trainer = PPOTrainer(config, model, tokenizer)\n",
    "trainer.step(queries, responses, rewards)\n",
    "```\n",
    "\n",
    "**Recommended starting point:** TRL library + Llama-3-8B or Mistral-7B.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: How expensive is this to train?\n",
    "\n",
    "**A:**\n",
    "\n",
    "| Scale | Cost | Hardware |\n",
    "|-------|------|----------|\n",
    "| GPT-4 level | $10M+ | 1000s of GPUs |\n",
    "| Fine-tune 7B model | $100-1000 | 1-8 GPUs |\n",
    "| This demo | Free | Your laptop |\n",
    "\n",
    "**Key insight:** You don't need to train from scratch! Fine-tuning a pre-trained model on specific tasks is much cheaper. The bottleneck is often **getting good reward signals**, not compute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
