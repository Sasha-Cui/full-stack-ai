{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f692dea",
   "metadata": {},
   "source": [
    "\n",
    "# DeepSpeed Tutorial\n",
    "\n",
    "Focus: **why DeepSpeed**, the core **ZeRO** idea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903ed02",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Section 1 ‚Äî Why DeepSpeed: The GPU Memory Problem in Training\n",
    "\n",
    "### 1) Motivation\n",
    "Training large models hits a **memory wall**. Even with big GPUs, the forward + backward pass stores many tensors simultaneously.\n",
    "\n",
    "### 2) What consumes memory during training\n",
    "| Component | What it is | Notes |\n",
    "|---|---|---|\n",
    "| **Model Parameters** | The trainable weights | ~1√ó model size |\n",
    "| **Activations** | Intermediates saved for backward | scales with batch√óseq√óhidden |\n",
    "| **Gradients** | Produced by backprop | ~1√ó model size |\n",
    "| **Optimizer States** | For Adam: momentum (m), variance (v) | ‚âà 2√ó model size |\n",
    "| **Temporary Buffers** | Workspace for matmuls/communication | dynamic overhead |\n",
    "\n",
    "With Adam, per-GPU memory can be **4‚Äì6√ó** the model size.\n",
    "\n",
    "### 3) Why na√Øve data parallel wastes memory\n",
    "Every GPU **replicates** params + grads + optimizer:\n",
    "```\n",
    "GPU0: params + grads + optimizer\n",
    "GPU1: params + grads + optimizer\n",
    "GPU2: params + grads + optimizer\n",
    "...\n",
    "```\n",
    "And the workflow is:\n",
    "1. Forward pass: each GPU computes loss on its data.\n",
    "2. Backward pass: each GPU computes local gradients.\n",
    "3. Gradients are **all-reduced** across GPUs ‚Üí everyone gets the same averaged gradients.\n",
    "4. Each GPU **updates** its own full parameter copy with its local optimizer.\n",
    "\n",
    "So 4 GPUs do **not** make a single GPU need 1/4th the memory ‚Äî each still holds everything.\n",
    "\n",
    "### 4) Why model parallel is NOT a good idea\n",
    "Model parallelism = splitting the computation across multiple GPUs, so that each GPU holds a different part of the model instead of a full copy. Each GPU **computes a subset of layers**.\n",
    "- **Manual partitioning** ‚Äì Layers or tensors must be explicitly assigned to GPUs.  \n",
    "- **High communication cost** ‚Äì GPUs must constantly exchange activations and gradients.  \n",
    "- **Sequential dependencies** ‚Äì Layers depend on outputs from previous GPUs, creating idle \"pipeline bubbles.\"  \n",
    "- **Backward complexity** ‚Äì Gradients must flow across devices, increasing synchronization overhead.  \n",
    "\n",
    "In short: This reduces both memory *and* compute per GPU, but adds cross-GPU communication for every layer.\n",
    "\n",
    "\n",
    "\n",
    "### 4) DeepSpeed‚Äôs core idea\n",
    "**ZeRO (Zero Redundancy Optimizer)** partitions these states **across** GPUs instead of replicating them, cutting memory per GPU roughly by the number of devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff19592",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è Section 2 ‚Äî ZeRO: The Heart of DeepSpeed\n",
    "\n",
    "### Big idea\n",
    "Don‚Äôt replicate all training states on each GPU; **shard** them. Communication reconstructs what‚Äôs needed on the fly.\n",
    "\n",
    "### ZeRO‚Äôs three stages\n",
    "\n",
    "#### üß© Stage 1 ‚Äî Shard optimizer states\n",
    "\n",
    "In optimizers like **Adam** or **AdamW**, at step $t$ each trainable parameter `Œ∏_i` has its own **state variables** that keep track of its historical updates ‚Äî for example:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_i^t &\\leftarrow \\beta_1 m_i^{t-1} + (1 - \\beta_1) g_i^t \\\\\n",
    "v_i^t &\\leftarrow \\beta_2 v_i^{t-1} + (1 - \\beta_2) (g_i^t)^2 \\\\\n",
    "\\theta_i^t &\\leftarrow \\theta_i^{t-1} - \\alpha \\frac{m_i^t / (1 - \\beta_1^t)}{\\sqrt{v_i^t / (1 - \\beta_2^t)} + \\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m_i $ = first moment (momentum term)\n",
    "- $ v_i $ = second moment (variance term)\n",
    "- $ g_i $ = gradient of that parameter\n",
    "- $ \\theta_i $ = parameter value itself\n",
    "\n",
    "Each parameter $ \\theta_i $ updates **only using its own** $ m_i $, $ v_i $, and $ g_i $.\n",
    "\n",
    "In vector form, Adam‚Äôs update is applied *elementwise*:\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\frac{m}{\\sqrt{v} + \\epsilon}$$\n",
    "\n",
    "So if we split the vector of parameters into chunks, each chunk can be updated **entirely on its own** ‚Äî as long as it has access to its local `m`, `v`, and `g`.\n",
    "\n",
    "Because the optimizer states are independent:\n",
    "- We can **shard** the `m` and `v` tensors across GPUs.\n",
    "- GPU 0 stores `m,v` for parameters [0 ‚Äì 25%], GPU 1 for [25 ‚Äì 50%], etc.\n",
    "- Each GPU updates *its subset* of parameters using its own local optimizer states.\n",
    "\n",
    "```\n",
    "GPU0: gets optimizer states[0‚Äì25%]\n",
    "GPU1: gets optimizer states[25‚Äì50%]\n",
    "GPU2: gets optimizer states[50‚Äì75%]\n",
    "GPU3: gets optimizer states[75‚Äì100%]\n",
    "```\n",
    "\n",
    "No communication is needed during the optimizer step except possibly for syncing the updated parameters after.\n",
    "\n",
    "\n",
    "##### Training Pipeline for ZeRO-1\n",
    "```\n",
    "üè≠ Forward (parameters duplicated) ‚Üí Backward (parameters, gradients duplicated)‚Üí All-Reduce (average gradients) ‚Üí Local Update (ONLY its shard of optimizer states) + Moving (m,v to others)\n",
    "```\n",
    "\n",
    "***Summary***\n",
    "- Shard Adam‚Äôs momentum/variance across GPUs.  \n",
    "- Full params + gradients remain replicated.  \n",
    "- **Save:** optimizer memory (‚âà 2√ó model size).  \n",
    "- **Comm:** moderate.\n",
    "\n",
    "#### üßÆ Stage 2 ‚Äî Shard gradients (plus optimizer)\n",
    "##### Idea: share graidents\n",
    "\n",
    "If each parameter has its own independent gradient, why should *every* GPU keep the *entire* gradient tensor?\n",
    "So ZeRO Stage 2 **partitions gradients** just like optimizer states:\n",
    "- Now, instead of everyone keeping the full `g_avg`, we **combine reduction and partitioning** in one step. Each GPU contributes its local gradients, but only receives the **shard** of the averaged result it needs.\n",
    "```\n",
    "GPU0: gets g_avg[0‚Äì25%]\n",
    "GPU1: gets g_avg[25‚Äì50%]\n",
    "GPU2: gets g_avg[50‚Äì75%]\n",
    "GPU3: gets g_avg[75‚Äì100%]\n",
    "```\n",
    "##### Why does this work? Independence of Gradients\n",
    "Each parameter `Œ∏_i` only needs its own gradient `g_i`.  \n",
    "No need to store gradients of other parameters.\n",
    "\n",
    "### üíæ Memory Benefit\n",
    "Each GPU stores only 1/N of gradients ‚Üí  **gradient memory reduced by factor N**.\n",
    "\n",
    "##### Training Pipeline for ZeRO-2\n",
    "```\n",
    "üè≠ Forward (parameters duplicated) ‚Üí Backward (parameters, gradients duplicated)‚Üí Reduce-scatter (average + partition grads across GPUs) ‚Üí Local Update (ONLY its shard of optimizer states) ‚Üí Broadcasting (updated parameters to others)\n",
    "```\n",
    "\n",
    "***Summary***\n",
    "- Gradients are partitioned using **reduce-scatter**.  \n",
    "- **Save:** optimizer + gradient memory.  \n",
    "- **Comm:** low.\n",
    "\n",
    "#### üß† Stage 3 ‚Äî Shard parameters (everything)\n",
    "##### Idea: share model parameters\n",
    "\n",
    "```\n",
    "GPU0: gets $\\theta$[0‚Äì25%]\n",
    "GPU1: gets $\\theta$[25‚Äì50%]\n",
    "GPU2: gets $\\theta$[50‚Äì75%]\n",
    "GPU3: gets $\\theta$[75‚Äì100%]\n",
    "```\n",
    "\n",
    "At any given moment, no GPU has the full model in memory.  \n",
    "Instead, parameters are **gathered just-in-time** when a layer needs them and **released** afterward.\n",
    "\n",
    "##### Implementation Detail: Parameter Flattening\n",
    "\n",
    "To avoid managing millions of small tensors,  \n",
    "ZeRO groups parameters into **flat contiguous memory chunks** called *parameter buckets* (or *flat buffers*).\n",
    "This eliminates almost all duplication ‚Äî memory per GPU ‚âà (1 / N) of the full model.\n",
    "\n",
    "##### Training Pipeline for ZeRO-3\n",
    "```\n",
    "üè≠ Forward (For each layer, GPUs **all-gather** the parameter shards required for that layer. Each GPU reconstructs the full layer weights *temporarily* in memory.) ‚Üí Backward ‚Üí Reduce-scatter (average + partition grads across GPUs) ‚Üí Local Update (ONLY its shard of optimizer states) + Broadcast (updated paramters to others)\n",
    "```\n",
    "\n",
    "***Summary***\n",
    "- Parameters themselves are partitioned and **gathered just-in-time** for compute.  \n",
    "- **Save:** optimizer + gradients + parameters.  \n",
    "- **Comm:** highest; enables training models larger than a single GPU‚Äôs memory.\n",
    "\n",
    "### Memory scaling (rule of thumb)\n",
    "| ZeRO Stage | What‚Äôs sharded | Memory reduction | \n",
    "|---|---|---|\n",
    "| 1 | Optimizer states | ~2√ó | \n",
    "| 2 | + Gradients | ~3√ó | \n",
    "| 3 | + Parameters | ~4‚Äì8√ó | \n",
    "\n",
    "### Illustration (conceptual)\n",
    "```\n",
    "Naive DP (replicated):   [full][full][full]\n",
    "ZeRO-1 (opt sharded):    [P,G, O‚ÇÅ][P,G, O‚ÇÇ][P,G, O‚ÇÉ]\n",
    "ZeRO-2 (opt+grad shard): [P, G‚ÇÅ,O‚ÇÅ][P, G‚ÇÇ,O‚ÇÇ][P, G‚ÇÉ,O‚ÇÉ]\n",
    "ZeRO-3 (all sharded):    [P‚ÇÅ,G‚ÇÅ,O‚ÇÅ][P‚ÇÇ,G‚ÇÇ,O‚ÇÇ][P‚ÇÉ,G‚ÇÉ,O‚ÇÉ]\n",
    "P=params, G=grads, O=optimizer shards\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
