{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0905bc",
   "metadata": {},
   "source": [
    "# vLLM + DeepSpeed\n",
    "\n",
    "```\n",
    "\n",
    "┌───────────────────────────────────────────────────────────────┐\n",
    "│                OUTER LOOP: RL iterations / epochs             │\n",
    "│                                                               │\n",
    "│   ┌───────────────┐      ┌───────────────────┐      ┌────────┐│\n",
    "│   │     vLLM      │ ───▶ │  Compute Rewards  │ ───▶ │DeepSpeed││\n",
    "│   │ Generate      │      │ (human/RM via     │      │ Train   ││\n",
    "│   │ Trajectories  │      │  vLLM if needed)  │      │ Policy   ││\n",
    "│   └──────┬────────┘      └─────────┬─────────┘      └────┬───┘│\n",
    "│          │                          │                    │    │\n",
    "│          └──────────────────────────┴────────────────────┘    │\n",
    "│                     (Updated Policy fed back)                 │\n",
    "└───────────────────────────────────────────────────────────────┘\n",
    "\n",
    "```\n",
    "\n",
    "## Motivation\n",
    "Imagine a reinforcement learning (RL) training loop: we **generate trajectories**, compute rewards, and **update the policy** model.  \n",
    "**vLLM** and **DeepSpeed** act as *system-level optimizers* for these two core stages.  \n",
    "vLLM accelerates **trajectory generation** and **reward inference**, making large-scale sampling highly efficient through optimized GPU memory management (PagedAttention).  \n",
    "DeepSpeed accelerates **policy training**, enabling distributed, memory-efficient optimization (via ZeRO) for massive models.  \n",
    "Together, they form the backbone of a scalable RL pipeline — vLLM for fast generation, DeepSpeed for efficient learning — both maximizing GPU utilization across the loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03987441",
   "metadata": {},
   "source": [
    "\n",
    "## 🧠 Section 1 — The KV Cache and Why It Matters\n",
    "\n",
    "\n",
    "### Attention Mechanism Formula\n",
    "\n",
    "The general form of the **scaled dot-product attention** is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ Q $ = Query matrix  \n",
    "- $ K $ = Key matrix  \n",
    "- $ V $ = Value matrix  \n",
    "- $ d_k $ = Dimension of the key vectors (used for scaling)\n",
    "\n",
    "At each decoding step, the model must compute attention between the **current query** and **all past key–value pairs** to determine what context is most relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Computational Cost Without Caching\n",
    "\n",
    "During **autoregressive generation**, each new token requires recomputing the attention using *all* previous tokens:\n",
    "\n",
    "$$\n",
    "\\text{Attention}_t = \\text{softmax}\\left(\\frac{Q_t K_{1:t}^T}{\\sqrt{d_k}}\\right)V_{1:t}\n",
    "$$\n",
    "\n",
    "That means for every new token $ t $, you rebuild $ K_{1:t} $ and $ V_{1:t} $ — leading to **quadratic time complexity** $ O(T^2) $ for a sequence of length $ T $.\n",
    "\n",
    "This becomes very expensive for long sequences.\n",
    "```\n",
    "\n",
    "            Generating the 4th token\n",
    "\n",
    "   ┌───────┬───────┬───────┐\n",
    "   │ x1    │ x2    │ x3    │\n",
    "   └──┬────┴──┬────┴──┬────┘\n",
    "      │       │       │\n",
    "      ▼       ▼       ▼\n",
    "     K1,V1   K2,V2   K3,V3      ← use Keys/Values from ALL previous tokens\n",
    "\n",
    "              ▲\n",
    "              │\n",
    "              │  Query from LAST hidden state (x3)\n",
    "              │\n",
    "             Q4\n",
    "\n",
    "   Attention: Q4 attends over {K1,K2,K3}\n",
    "   ↓\n",
    "   Produces next representation → predict token 4\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "### 3. Introducing the KV Cache\n",
    "\n",
    "To avoid recomputing the same $ K $ and $ V $ at every step, we **cache** them:\n",
    "\n",
    "$$\n",
    "K_{1:t} = [K_{1:t-1}; K_t], \\quad V_{1:t} = [V_{1:t-1}; V_t]\n",
    "$$\n",
    "\n",
    "At each new step:\n",
    "- The model **reuses** the previously stored keys and values.\n",
    "- Only the **new** $ K_t $ and $ V_t $ for the latest token are computed and appended to the cache.\n",
    "\n",
    "This allows attention to be computed efficiently as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}_t = \\text{softmax}\\left(\\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}}\\right)V_{\\text{cache}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Resulting Benefits\n",
    "\n",
    "- ✅ **Reduces computation** — only one step of attention per new token.\n",
    "- ✅ **Reduces memory access** — no need to rebuild past hidden states.\n",
    "- ✅ **Enables fast autoregressive decoding**, making real-time generation (like chat or translation) feasible.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Summary Table\n",
    "\n",
    "| Step | Without KV Cache | With KV Cache |\n",
    "|------|------------------|---------------|\n",
    "| Compute Keys/Values | Recomputed each step | Reused from cache |\n",
    "| Complexity | $ O(T^2) $ | $ O(T) $ |\n",
    "| Speed | Slow for long sequences | Much faster |\n",
    "\n",
    "\n",
    "### 6. Throughput via Batching, and Its Two Problems\n",
    "To use the GPU efficiently, we **batch** requests. But in practice:\n",
    "1) **Asynchronous arrivals:** requests don’t start together.  \n",
    "2) **Variable lengths:** prompts/outputs differ; some finish early.\n",
    "\n",
    "This leads to idle threads and poor packing unless we **rebatch every iteration** (a.k.a. *iteration-level scheduling* / *cellular batching*).\n",
    "\n",
    "### 7. KV Cache Still Has Issues\n",
    "Even with better batching, many concurrent, variable-length requests cause **KV cache memory fragmentation**, turning into a GPU memory management problem that Section 2 addresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9197680",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ Section 2 — PagedAttention: Solving the KV Cache Memory Problem\n",
    "\n",
    "### 1. Motivation: KV Cache Fixes Computation, but Creates a Memory Challenge\n",
    "KV caching saves compute but each request’s cache **grows autoregressively** as new tokens are generated. With many concurrent requests, caches grow and finish at different times, stressing GPU memory.\n",
    "\n",
    "### 2. The Naïve Way: Consecutive (Contiguous) Allocation\n",
    "```\n",
    "┌───────────────────────────────────────────────┐\n",
    "│ [Req A (50 tok)] [Req B (30 tok)] [Req C (80)]│\n",
    "└───────────────────────────────────────────────┘\n",
    "```\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│ [Req A (50 tok)] [cleared (30 tok)] [Req C (80)] [Req D (60)]│\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "Each request holds one **contiguous** KV tensor (≈ `[layers, heads, tokens, head_dim]`). When **A** generates the next token, it must append immediately after its block — but **B** is already there. Growing A would require shifting/copying huge tensors and stalling kernels.\n",
    "\n",
    "Finished requests create **holes** that are often unusable for new requests → **fragmentation**.\n",
    "\n",
    "### 3. The “Cut It Up” Idea — Why It’s Hard\n",
    "Splitting a request’s KV into smaller chunks would pack memory better, but then the attention kernel must know **where every token’s K/V lives** and reconstruct the correct order — hard to track without structure.\n",
    "```\n",
    "┌───────────────────────────────────────────────┐\n",
    "│ [Req A (50 tok)] [Req B (30 tok)] [Req C (80)]│\n",
    "└───────────────────────────────────────────────┘\n",
    "```\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ [Req A (50 tok)] [Req D part 1 (30 tok)] [Req C (80)] [Req D part 2 (30)]│\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 4. The Breakthrough: PagedAttention\n",
    "**PagedAttention** slices GPU memory into **uniform pages** (small fixed-size blocks, e.g., 16 tokens per page). Each request’s KV cache is a list of pages, not one big block.\n",
    "\n",
    "```\n",
    "GPU pages:  P1 | P2 | P3 | P4 | P5 | P6 | ...\n",
    "Req A  → P1,P2\n",
    "Req B  → P3\n",
    "Req C  → P4,P5\n",
    "A grows → take P6 (no moves, just add a page)\n",
    "```\n",
    "\n",
    "### 5. The Page Table (Indirection)\n",
    "A lightweight **page table** maps *logical token indices* → *(page, offset)*:\n",
    "```\n",
    "token 37 → page_table[37//page_size] + (37 % page_size)\n",
    "```\n",
    "Attention kernels read K/V via this mapping, so KV can be physically scattered yet **logically contiguous**.\n",
    "\n",
    "### 6. Why This Optimizes Memory Use\n",
    "- **No fragmentation:** freed pages are uniform and immediately reusable\n",
    "- **No large copies:** growing requests just add pages; finishing requests return pages\n",
    "- **Concurrency-friendly:** requests grow/finish independently\n",
    "- **Virtual-memory-like behavior:** logical order preserved via page table\n",
    "\n",
    "### 7. Summary Visualization\n",
    "```\n",
    "Without paging (contiguous): AAAAAA BBB … CCCCC D …\n",
    "→ gaps, blocked growth, relocations\n",
    "\n",
    "With PagedAttention: P1(A) P2(A) P3(B) P4(C) P5(Anew) …\n",
    "→ reuse pages, no relocations, steady throughput\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ac3f7",
   "metadata": {},
   "source": [
    "\n",
    "## 🏗️ Section 3 — Inside vLLM: How Paging and Scheduling Work Together\n",
    "\n",
    "### 1. Goal\n",
    "Keep the GPU highly utilized despite **asynchronous arrivals** and **variable-length** requests.\n",
    "\n",
    "### 2. Runtime Pipeline\n",
    "```\n",
    "User/API → Request Queue → Scheduler (per-iteration) → GPU Workers → PagedAttention → Streams Out\n",
    "```\n",
    "\n",
    "- **Request Queue:** holds incoming work\n",
    "- **Scheduler:** mixes new and ongoing requests instead of waiting for all to finish before starting new ones. It collects all active sequences that need their next token. It admits new requests if there’s free KV-cache memory. It removes finished sequences (those that reached EOS or max length).\n",
    "- **GPU Workers:** run attention kernels; read K/V via page table; write new K/V into free pages\n",
    "- **PagedAttention:** provides flexible KV memory without copies\n",
    "- **Streaming:** return tokens as they’re ready while continuing decoding\n",
    "\n",
    "### 3. Why This Works\n",
    "- PagedAttention allows **dynamic batching** without moving memory\n",
    "- The scheduler keeps batches full each iteration\n",
    "- Memory stays compact and reusable; fewer OOMs and higher throughput\n",
    "\n",
    "### 4. Concept Diagram\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│ [Incoming Requests] → Queue → Scheduler → GPU Worker → PagedAttention →  │\n",
    "│                             ↑                                   ↓        │\n",
    "│                         Stream tokens back to clients continuously       │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9b4ca",
   "metadata": {},
   "source": [
    "\n",
    "## 🚀 Section 4 — Minimal “How to Run” Demos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b444fe4",
   "metadata": {},
   "source": [
    "### 4.1 Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fresh environment recommended\n",
    "# If you're in Jupyter, the leading '%' is allowed; in plain Python, remove it.\n",
    "%pip install -U vllm transformers accelerate openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596c9d7",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Pick a model\n",
    "`Qwen3-4B-Instruct-2507`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load model with simpler configuration\n",
    "model = \"/gpfs/radev/project/zhuoran_yang/xh338/models/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Try with minimal settings\n",
    "llm = LLM(\n",
    "    model=model,\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=2048,  # Reduce max length\n",
    "    enforce_eager=True   # Disable compilation\n",
    ")\n",
    "\n",
    "# Test with simple prompts\n",
    "prompts = [\n",
    "    \"<|im_start|>user\\nHi, how are you?<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"<|im_start|>user\\nTell me a short story about a robot.<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"<|im_start|>user\\nExplain what AI is in simple terms.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "]\n",
    "\n",
    "params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=200,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "outputs = llm.generate(prompts, params)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    response = output.outputs[0].text\n",
    "    print(f\"Response {i+1}: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Cursor)",
   "language": "python",
   "name": "python311-cursor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
