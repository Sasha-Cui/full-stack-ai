{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Evaluation & Alignment\n",
    "\n",
    "**Chris Zhu**  \n",
    "Yale University | Becoming Full-Stack AI Researchers Working Group  \n",
    "November 4, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The LLM Benchmark Landscape \n",
    "\n",
    "### 1.1 What is an LLM Benchmark?\n",
    "\n",
    "A benchmark is a **standardized test** designed to measure specific capabilities of language models:\n",
    "\n",
    "- What can the model do?\n",
    "- How well does it perform?\n",
    "- Where does it fail?\n",
    "- How does it compare to others?\n",
    "\n",
    "**Key insight:** Benchmarks shape how we understand progress in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploring the Ecosystem: Epoch AI Benchmarks\n",
    "\n",
    "Let's look at the current landscape through [Epoch AI's Benchmark Database](https://epoch.ai/benchmarks):\n",
    "\n",
    "**Major Categories:**\n",
    "\n",
    "| Category | Examples | What they measure |\n",
    "|----------|----------|------------------|\n",
    "| Knowledge & Reasoning | MMLU, BBH, ARC | Factual knowledge, logical reasoning |\n",
    "| Math & Code | GSM8K, MATH, HumanEval | Quantitative reasoning, programming |\n",
    "| Language Understanding | HellaSwag, PIQA | Commonsense, context understanding |\n",
    "| Multi-modal | MMMU, VQA | Vision-language integration |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The Core Problem: Saturation\n",
    "\n",
    "**Three fundamental issues with traditional benchmarks:**\n",
    "\n",
    "1. **Rapid saturation** - Models quickly reach near-perfect scores\n",
    "2. **Data contamination** - Test sets leak into training data\n",
    "3. **Lack of construct validity** - High scores don't always mean genuine capability\n",
    "\n",
    "**Example:** GPT-4 exceeds human performance on MMLU, but does this mean it's \"smarter\" than humans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Modern Evaluation Frameworks\n",
    "\n",
    "**Key packages in the ecosystem:**\n",
    "\n",
    "| Package | Organization | Key Feature |\n",
    "|---------|-------------|-------------|\n",
    "| **lm-eval-harness** | EleutherAI | Academic benchmarks, 200+ tasks, unified interface |\n",
    "| **HELM** | Stanford | Holistic metrics across scenarios, Capabilities + bias + safety + efficiency |\n",
    "| **inspect_ai** | UK AISI | Behavioral & agent evals, Trace-level analysis, deception/safety experiments|\n",
    "| **OpenAI Evals** | OpenAI | Production-ready, community-driven, CI-style evaluation, LLM-as-judge |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04:16:42:04,136 INFO     [huggingface.py:162] Using device 'mps'\n",
      "2025-11-04:16:42:06,575 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "Running loglikelihood requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 5429/7997 [37:56<17:56,  2.38it/s]\n",
      "2025-11-04:16:42:12,925 WARNING  [evaluator.py:222] Overwriting default num_fewshot of arc_easy from None to 0\n",
      "2025-11-04:16:42:12,925 WARNING  [evaluator.py:222] Overwriting default num_fewshot of hellaswag from None to 0\n",
      "2025-11-04:16:42:12,926 INFO     [task.py:395] Building contexts for arc_easy on rank 0...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2980.01it/s]\n",
      "2025-11-04:16:42:12,964 INFO     [task.py:395] Building contexts for hellaswag on rank 0...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 7447.14it/s]\n",
      "2025-11-04:16:42:12,987 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 799/799 [00:11<00:00, 69.02it/s] \n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAW RESULTS KEYS ===\n",
      "    arc_easy -> ['acc,none', 'acc_stderr,none', 'acc_norm,none', 'acc_norm_stderr,none', 'alias']\n",
      "   hellaswag -> ['acc,none', 'acc_stderr,none', 'acc_norm,none', 'acc_norm_stderr,none', 'alias']\n",
      "\n",
      "=== RAW RESULTS (truncated) ===\n",
      "{'arc_easy': {'acc,none': 0.38,\n",
      "              'acc_norm,none': 0.34,\n",
      "              'acc_norm_stderr,none': 0.04760952285695233,\n",
      "              'acc_stderr,none': 0.04878317312145634},\n",
      " 'hellaswag': {'acc,none': 0.36,\n",
      "               'acc_norm,none': 0.44,\n",
      "               'acc_norm_stderr,none': 0.049888765156985884,\n",
      "               'acc_stderr,none': 0.048241815132442176}}\n",
      "\n",
      " Evaluation Results\n",
      "hellaswag: 36.0%  (acc,none)\n",
      "arc_easy: 38.0%  (acc,none)\n",
      "\n",
      " Demo complete.\n",
      "\n",
      "ðŸ“ˆ Figure saved to: results/lm_eval_demo_plot.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust lm-eval-harness demo:\n",
    "- Runs GPT-2 on two classic reasoning benchmarks:\n",
    "    * HellaSwag: commonsense completion (choose most plausible ending)\n",
    "    * ARC-Easy: grade-school science questions (multiple choice)\n",
    "- Works on CPU or Apple Silicon (MPS)\n",
    "- Prints RAW metric keys for transparency\n",
    "- Extracts accuracy cleanly across lm-eval versions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "# Avoid CUDA probing and silence tokenizers fork warnings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from lm_eval import simple_evaluate\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "\n",
    "# Choose execution device:\n",
    "# - Prefer MPS (Apple Silicon GPU) if available; otherwise use CPU\n",
    "device = \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Load a small local model (no API keys required)\n",
    "model = HFLM(\n",
    "    pretrained=\"gpt2\",\n",
    "    device=device,\n",
    "    dtype=\"float32\",        # safe, portable precision for CPU/MPS\n",
    "    use_fast_tokenizer=True\n",
    ")\n",
    "\n",
    "# Run a reasonably fast demo (increase/remove `limit` for full eval)\n",
    "results = simple_evaluate(\n",
    "    model=model,\n",
    "    tasks=[\"hellaswag\", \"arc_easy\"],\n",
    "    num_fewshot=0,          # zero-shot\n",
    "    batch_size=8,\n",
    "    limit=100,             # demo speed; set to None for full benchmark\n",
    ")\n",
    "\n",
    "# --- Inspect raw results so we know which metric keys exist ---\n",
    "print(\"\\n=== RAW RESULTS KEYS ===\")\n",
    "for task, vals in results[\"results\"].items():\n",
    "    print(f\"{task:>12} -> {list(vals.keys())}\")\n",
    "print(\"\\n=== RAW RESULTS (truncated) ===\")\n",
    "pprint({k: {kk: vv for kk, vv in v.items() if isinstance(vv, (int, float))}\n",
    "        for k, v in results[\"results\"].items()})\n",
    "\n",
    "# --- Flexible metric extraction (covers common variants across versions) ---\n",
    "def get_metric(res, task):\n",
    "    \"\"\"\n",
    "    Return (value, key) for the main 'accuracy-like' metric if found.\n",
    "    Falls back to the first numeric ratio in [0,1].\n",
    "    \"\"\"\n",
    "    task_res = res[\"results\"].get(task, {})\n",
    "    preferred = (\"acc_norm\", \"acc\", \"accuracy\", \"multiple_choice_grade\", \"acc,none\")\n",
    "    for k in preferred:\n",
    "        v = task_res.get(k, None)\n",
    "        if isinstance(v, (int, float)) and 0.0 <= v <= 1.0:\n",
    "            return v, k\n",
    "    # Fallback: any numeric ratio in [0,1]\n",
    "    for k, v in task_res.items():\n",
    "        if isinstance(v, (int, float)) and 0.0 <= v <= 1.0:\n",
    "            return v, k\n",
    "    return None, None\n",
    "\n",
    "def display(task):\n",
    "    val, key = get_metric(results, task)\n",
    "    if val is None:\n",
    "        print(f\"{task}: â“ metric unavailable (see RAW RESULTS KEYS above)\")\n",
    "    else:\n",
    "        print(f\"{task}: {val:.1%}  ({key})\")\n",
    "\n",
    "print(\"\\n Evaluation Results\")\n",
    "display(\"hellaswag\")\n",
    "display(\"arc_easy\")\n",
    "print(\"\\n Demo complete.\\n\")\n",
    "\n",
    "# --- quick plot  ---\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    pairs = []\n",
    "    for t in (\"hellaswag\", \"arc_easy\"):\n",
    "        v, k = get_metric(results, t)\n",
    "        if v is not None:\n",
    "            pairs.append((t, v * 100.0, k))\n",
    "    if pairs:\n",
    "        plt.figure(figsize=(5, 3.2))\n",
    "        xs = [p[0] for p in pairs]\n",
    "        ys = [p[1] for p in pairs]\n",
    "        bars = plt.bar(xs, ys)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title(\"lm-eval-harness (GPT-2 demo)\")\n",
    "        for b, pct in zip(bars, ys):\n",
    "            plt.text(b.get_x() + b.get_width()/2, b.get_height() + 1, f\"{pct:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        out_path = \"results/lm_eval_demo_plot.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        print(f\"ðŸ“ˆ Figure saved to: {out_path}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âš ï¸ No numeric accuracy metrics available to plot.\")\n",
    "except Exception as e:\n",
    "    print(f\"Plot skipped ({e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Case Study: SWE-bench Evolution\n",
    "\n",
    "**SWE-bench** (Software Engineering Benchmark) - A real-world coding benchmark\n",
    "\n",
    "**Live site:** https://www.swebench.com/\n",
    "\n",
    "**Original Version (Princeton, 2024):**\n",
    "- Task: Solve real GitHub issues from 12 Python repositories\n",
    "- Dataset: 2,294 issue-pull request pairs\n",
    "- Metric: Can the model generate a patch that passes all tests?\n",
    "- Initial result: GPT-4 achieved only **1.7% success rate**\n",
    "\n",
    "**Why so low?** This is a genuinely hard task requiring:\n",
    "- Understanding large codebases\n",
    "- Debugging complex issues\n",
    "- Generating working code patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 SWE-bench: The Refinement Cycle\n",
    "\n",
    "**Problems discovered with the original benchmark:**\n",
    "\n",
    "1. **Data contamination** - Models may have seen GitHub repositories during pre-training\n",
    "2. **Instance quality issues** - A subset of GitHub issues were **underspecified or ambiguous**, and some test suites had **flake or setup quirks**\n",
    "3. **Evaluation noise** - Docker/test execution environment sometimes produced **non-deterministic outcomes** \n",
    "\n",
    "**SWE-bench Lite (Princeton, 2024):**\n",
    "- Curated subset of 300 high-quality, verified instances\n",
    "- Removed problems with ambiguous specifications\n",
    "- More stable test execution\n",
    "- GPT-4: ~3-5% (still challenging)\n",
    "\n",
    "**SWE-bench Verified (OpenAI, 2024):**\n",
    "- Manual verification of each instance\n",
    "- Human expert validation\n",
    "- 500 instances with clear acceptance criteria\n",
    "- Added human baseline for comparison\n",
    "\n",
    "**The pattern:** Even carefully designed benchmarks need iterative refinement based on real-world usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: DealBench - Adversarial Evaluation\n",
    "\n",
    "### 2.1 The Paradigm Shift: From Static to Adversarial\n",
    "\n",
    "**Why do benchmarks keep failing?**\n",
    "\n",
    "| Traditional Benchmarks | Problem |\n",
    "|----------------------|----------|\n",
    "| Fixed answer keys | Memorization, data leakage |\n",
    "| Single-turn evaluation | Misses interactive intelligence |\n",
    "| Static difficulty | Rapid saturation |\n",
    "\n",
    "**What if we design a benchmark that:**\n",
    "- Has **no fixed answers** (outcome depends on opponent)\n",
    "- Measures **strategic interaction** (multi-turn, adaptive)\n",
    "- **Resists saturation** (difficulty scales with opponent strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Introducing DealBench\n",
    "\n",
    "**DealBench:** An adversarial negotiation benchmark built on realistic MBA business cases.\n",
    "\n",
    "**Core idea:** Evaluate models by having them negotiate with each other.\n",
    "\n",
    "**Why negotiation?**\n",
    "- Clear economic outcomes (measurable value creation)\n",
    "- No single \"correct\" answer (opponent-dependent)\n",
    "- Tests strategic reasoning, theory of mind, multi-turn planning\n",
    "- Exposes safety-relevant behaviors (lying, rule-breaking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Formal Framework\n",
    "\n",
    "Let's formalize a negotiation:\n",
    "\n",
    "**Setup:**\n",
    "- Issue space: $\\Omega$ (e.g., salary, start date, location, bonuses)\n",
    "- Utility functions: $u_i : \\Omega \\to \\mathbb{R}$ for each party $i$\n",
    "- BATNA (Best Alternative to Negotiated Agreement): $b_i$ = value of walking away\n",
    "\n",
    "**Zone of Possible Agreement (ZOPA):**\n",
    "\n",
    "$$\\mathcal{Z} = \\{\\omega \\in \\Omega : u_i(\\omega) > b_i \\text{ for all } i\\}$$\n",
    "\n",
    "**Total Pie (value created):**\n",
    "\n",
    "$$P(\\omega) = \\sum_i (u_i(\\omega) - b_i)$$\n",
    "\n",
    "**Pie Fraction (relative share):**\n",
    "\n",
    "$$p_i(\\omega) = \\frac{u_i(\\omega) - b_i}{P(\\omega)} \\in [0,1], \\quad \\sum_i p_i(\\omega) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Three Canonical Negotiation Types\n",
    "\n",
    "**1. No-ZOPA Case**\n",
    "\n",
    "- Scenario: A historic property where buyer wants redevelopment but seller requires preservation\n",
    "- $\\mathcal{Z} = \\emptyset$ (no mutually acceptable deal exists)\n",
    "- **Test:** Can the model recognize when no deal is possible?\n",
    "\n",
    "---\n",
    "\n",
    "**2. Zero-Sum Single-Issue**\n",
    "\n",
    "- Scenario: Contractor vs homeowner negotiating price for tree removal\n",
    "- Only one issue: price\n",
    "- $\\sum_i u_i(\\omega) = \\text{constant}$ (pure distributive bargaining)\n",
    "- **Test:** Can the model claim value effectively?\n",
    "\n",
    "---\n",
    "\n",
    "**3. Multi-Issue Positive-Sum**\n",
    "\n",
    "<img src=\"./figures/muiti_issue.png\" width=\"700\">\n",
    "\n",
    "- Scenario: Job offer negotiation (salary, start date, location, bonuses, division)\n",
    "- Multiple issues with different priorities for each party\n",
    "- Potential for \"elegant trades\" that expand total value\n",
    "- **Test:** Can the model discover integrative solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluation Methodology: Two Types of Play\n",
    "\n",
    "**We evaluate models using two complementary approaches:**\n",
    "\n",
    "#### 2.5.1 Mirror Play (Model vs Itself)\n",
    "\n",
    "**Setup:**\n",
    "- Same model plays both sides of the negotiation\n",
    "- Tests value creation ability in a controlled setting\n",
    "- Used for comparison with human baseline\n",
    "\n",
    "**Key Metric: Total Pie**\n",
    "$$P(\\omega) = \\sum_i (u_i(\\omega) - b_i)$$\n",
    "How much joint value is created above both parties' BATNAs?\n",
    "\n",
    "**Benchmark: MBA Students**\n",
    "- Collected human baseline on identical negotiation cases\n",
    "- MBA students at Yale School of Management\n",
    "- Same scenarios, same constraints, same evaluation\n",
    "\n",
    "**Finding:** Top models significantly outperform MBA students\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.5.2 Cross Play (Model vs Model)\n",
    "\n",
    "**Setup:**\n",
    "- Every model negotiates against every other model\n",
    "- Tests strategic adaptation and competitive performance\n",
    "- Reveals relative strengths across different opponents\n",
    "\n",
    "**Key Metric: Pie Fraction**\n",
    "$$p_i(\\omega) = \\frac{u_i(\\omega) - b_i}{P(\\omega)}$$\n",
    "What share of the jointly created value does each model capture?\n",
    "\n",
    "**Purpose:**\n",
    "- Rank models by negotiation skill\n",
    "- Identify which models are better at claiming value\n",
    "- Test robustness across diverse opponents\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Why both metrics matter:**\n",
    "\n",
    "| Metric | What it measures | Economic interpretation |\n",
    "|--------|------------------|------------------------|\n",
    "| Total Pie | Joint value creation | Efficiency (finding win-win solutions) |\n",
    "| Pie Fraction | Value distribution | Bargaining power (claiming your share) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Live Demo\n",
    "\n",
    "**Let's see DealBench in action!**\n",
    "\n",
    "I'll now switch to the terminal and run:\n",
    "\n",
    "```bash\n",
    "python -m negbench\\\n",
    "  --scenario scenarios/SnyderMed.yaml \\\n",
    "  --n-runs 1 \\\n",
    "  --rounds 6 \\\n",
    "  --models anthropic/claude-3.5-sonnet,openai/gpt-4o \\\n",
    "  --no-use-plan \\\n",
    "  --no-use-memory \\\n",
    "  --exp-name 2025-11-04_Demo \\\n",
    "  --verbose\n",
    "```\n",
    "\n",
    "**What you'll observe:**\n",
    "- Two LLMs negotiating a job offer\n",
    "- Multi-turn offers and counteroffers\n",
    "- Deal closure or walk-away\n",
    "- Automatic value calculation and compliance checks\n",
    "\n",
    "[LIVE DEMO SECTION - TERMINAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: From Evaluation to Alignment \n",
    "\n",
    "### 3.1 From Measuring Quality to Improving Quality\n",
    "\n",
    "**So far, we've focused on evaluation:**\n",
    "- How do we measure if a model is good?\n",
    "- Traditional benchmarks: fixed tests\n",
    "- DealBench: adversarial interaction\n",
    "\n",
    "**Now, a natural next question:**\n",
    "- How do we make models better?\n",
    "- Specifically: How do we train models to be helpful, harmless, and honest?\n",
    "\n",
    "**In alignment:**\n",
    "- Which summary is more helpful? â†’ Depends on user needs\n",
    "- Is this response appropriate? â†’ Depends on context and values\n",
    "\n",
    "**The insight:** If we can systematically judge preferences, we can use those judgments to improve models.\n",
    "\n",
    "**This is the foundation of modern alignment techniques: RLHF and RLAIF.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Alignment Problem\n",
    "\n",
    "**Goal:** Train models to produce outputs that humans prefer.\n",
    "\n",
    "**Challenge:** \n",
    "- Preference is subjective (what's \"helpful\" or \"harmless\"?)\n",
    "- Hard to specify formally\n",
    "- Context-dependent\n",
    "\n",
    "**Traditional approach: RLHF (Reinforcement Learning from Human Feedback)**\n",
    "\n",
    "1. Collect human preference labels on pairs of model outputs\n",
    "2. Train a reward model to predict human preferences\n",
    "3. Use RL to optimize policy against the reward model\n",
    "\n",
    "**The bottleneck:** Human annotation is expensive and slow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 RLAIF: RL from AI Feedback\n",
    "\n",
    "**Core question:** Can we use a strong LLM to generate preference labels instead of humans?\n",
    "\n",
    "<img src=\"./figures/RLHF.png\" width=\"1000\">\n",
    "\n",
    "**RLAIF Pipeline:**\n",
    "\n",
    "1. Start with supervised fine-tuned (SFT) model\n",
    "2. Generate response pairs\n",
    "3. **AI labeler** (e.g., PaLM 2 Large) judges which response is better\n",
    "4. Train reward model on AI preferences\n",
    "5. Run RL to optimize policy\n",
    "\n",
    "**Key difference:** Replace human annotators with an off-the-shelf LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 How to Get AI Preferences\n",
    "\n",
    "**Prompt structure for AI labeler:**\n",
    "\n",
    "**Step 1: Preamble**  \n",
    "*A good summary is concise, accurate, and covers key information...*\n",
    "\n",
    "**Step 2: Sample to Annotate**  \n",
    "- Text: [original Reddit post]  \n",
    "- Summary 1: [candidate A]  \n",
    "- Summary 2: [candidate B]\n",
    "\n",
    "**Step 3: Chain-of-Thought Prompt**  \n",
    "*Consider coherence, accuracy, coverage. Explain which summary is better.*\n",
    "\n",
    "**Step 4: LLM Output**  \n",
    "*Rationale: [LLM generates reasoning explaining its preference]*\n",
    "\n",
    "**Step 5: Final Preference**  \n",
    "*Preferred Summary: [1 or 2]*\n",
    "\n",
    "---\n",
    "\n",
    "**Key techniques:**\n",
    "- **Chain-of-thought reasoning** improves alignment (+1.9%)\n",
    "- **Position debiasing** (run twice with reversed order)\n",
    "- **Extract log-probabilities** for soft labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Mathematical Framework: From Preferences to Aligned Models\n",
    "\n",
    "Now that we have AI-generated preferences, how do we use them to train better models?\n",
    "\n",
    "**Two-stage process:**\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 1: Training the Reward Model\n",
    "\n",
    "**Goal:** Learn to predict which response is better\n",
    "\n",
    "**Input:** Preference dataset $\\mathcal{D} = \\{(x, y_w, y_l)\\}$ where:\n",
    "- $x$ = prompt (e.g., \"Summarize this article\")\n",
    "- $y_w$ = preferred response (higher quality)\n",
    "- $y_l$ = less preferred response (lower quality)\n",
    "\n",
    "**Model:** Train a reward function $r_\\phi(x, y)$ that assigns scores to responses\n",
    "\n",
    "**Training objective:**\n",
    "\n",
    "$$\\mathcal{L}_r(\\phi) = -\\mathbb{E}_{(x,y_w,y_l)\\sim\\mathcal{D}}\\left[\\log \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))\\right]$$\n",
    "\n",
    "**Intuition:** Maximize the probability that $r(y_w) > r(y_l)$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "| | Prompt | Response | Reward Score |\n",
    "|---|--------|----------|--------------|\n",
    "| **Preferred** | \"Write a polite email\" | *\"Dear Sir, I hope this email finds you well...\"* | $r = 2.3$ â¬† |\n",
    "| **Not preferred** | \"Write a polite email\" | *\"Hey, here's what I need...\"* | $r = -0.8$ â¬‡ |\n",
    "\n",
    "After training, the reward model learns that formal language gets higher scores.\n",
    "\n",
    "---\n",
    "\n",
    "#### Stage 2: Reinforcement Learning with KL Penalty\n",
    "\n",
    "**Goal:** Train policy to generate high-reward (= high-quality) responses\n",
    "\n",
    "**Challenge:** If we only maximize reward, the model might \"cheat\"\n",
    "\n",
    "**Why KL Penalty Matters: Preventing Reward Hacking**\n",
    "\n",
    "Consider what could go wrong without constraints:\n",
    "\n",
    "| Scenario | Model Output | Reward Model | Reality |\n",
    "|----------|--------------|--------------|---------|\n",
    "| **Without penalty** | *\"This AMAZING article presents INCREDIBLE findings with PHENOMENAL clarity!!!\"* | High score  (positive words) |  Useless word salad |\n",
    "| **With KL penalty** | *\"The study found three key results: X, Y, and Z\"* | Good score  |  Actually helpful |\n",
    "\n",
    "**Solution:** Add a KL divergence penalty to keep the model reasonable\n",
    "\n",
    "**RL objective:**\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{y\\sim\\pi_\\theta(\\cdot|x)}\\left[\\underbrace{(1-\\beta)r_\\phi(y|x)}_{\\text{maximize reward}} - \\underbrace{\\beta \\cdot D_{KL}(\\pi_\\theta \\| \\pi_{\\text{SFT}})}_{\\text{stay close to SFT}}\\right]$$\n",
    "\n",
    "**Components explained:**\n",
    "\n",
    "| Symbol | Meaning | Intuition |\n",
    "|--------|---------|-----------|\n",
    "| $\\pi_\\theta$ | Policy being trained | The model we're improving |\n",
    "| $r_\\phi(y\\|x)$ | Reward for response $y$ | How good is this response? |\n",
    "| $\\pi_{\\text{SFT}}$ | Original SFT model | Safe baseline model |\n",
    "| $D_{KL}$ | KL divergence | How much has the model changed? |\n",
    "| $\\beta$ | Penalty coefficient | Controls the trade-off (typically 0.05) |\n",
    "\n",
    "**The trade-off:**\n",
    "\n",
    "- **High reward term $(1-\\beta)r_\\phi$:** Generate responses humans prefer\n",
    "- **KL penalty $-\\beta D_{KL}$:** Don't drift too far from the safe baseline\n",
    "\n",
    "**Why KL divergence matters:**\n",
    "\n",
    "$$D_{KL}(\\pi_\\theta \\| \\pi_{\\text{SFT}}) = \\mathbb{E}_y\\left[\\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{SFT}}(y|x)}\\right]$$\n",
    "\n",
    "- Small $D_{KL}$ â†’ Model stays similar to SFT (conservative, safe)\n",
    "- Large $D_{KL}$ â†’ Model becomes very different (risky, might generate nonsense)\n",
    "\n",
    "---\n",
    "\n",
    "#### Choosing $\\beta$: The Safety-Performance Trade-off\n",
    "\n",
    "| $\\beta$ | Behavior | Example |\n",
    "|---------|----------|---------|\n",
    "| 0.0 | Pure reward maximization | \"BEST SUMMARY EVER!!!\" (reward hacking) |\n",
    "| 0.05 | **Balanced (typical choice)** | Good improvements, stays reasonable |\n",
    "| 0.2 | Conservative | Safe but limited improvement |\n",
    "| 1.0 | No change from SFT | Ignores reward completely |\n",
    "\n",
    "**In the RLAIF paper:** $\\beta = 0.05$ works well across all tasks\n",
    "\n",
    "---\n",
    "\n",
    "#### Training Algorithm\n",
    "\n",
    "**Objective at each step:**\n",
    "\n",
    "$$J(\\theta) = (1-\\beta) r_\\phi(x,y) - \\beta \\cdot D_{KL}(\\pi_\\theta \\| \\pi_{\\text{SFT}})$$\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Sample prompt $x$ from dataset\n",
    "2. Generate response $y \\sim \\pi_\\theta(\\cdot|x)$ from current policy\n",
    "3. Score with reward model: $r_\\phi(x, y)$\n",
    "4. Compute KL divergence: $D_{KL}(\\pi_\\theta \\| \\pi_{\\text{SFT}})$\n",
    "5. Update parameters: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$\n",
    "6. Repeat for approximately 10,000 steps\n",
    "\n",
    "**Typical hyperparameters:** $\\beta = 0.05$, learning rate $\\alpha = 10^{-5}$\n",
    "\n",
    "**Result:** A model that generates high-quality responses while staying grounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Main Result: RLAIF Matches RLHF\n",
    "\n",
    "\n",
    "**Experiment:** Train models on three tasks and compare final outputs via human evaluation\n",
    "\n",
    "\n",
    "| Task | RLAIF vs SFT | RLHF vs SFT | RLAIF vs RLHF |\n",
    "|------|--------------|-------------|---------------|\n",
    "| Summarization | 71% | 73% | 50% (tie) |\n",
    "| Helpful Dialogue | 63% | 64% | 52% (tie) |\n",
    "| Harmless Dialogue | 88% | 76% | **RLAIF wins** |\n",
    "\n",
    "**Key finding:** AI feedback is competitive with human feedback.\n",
    "\n",
    "**Implications:**\n",
    "- 10x cheaper than human annotation\n",
    "- Faster iteration cycles\n",
    "- Scales better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**The bigger picture:** We're moving toward **self-improving, adversarially-tested AI systems**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "### Core Papers\n",
    "\n",
    "1. **SWE-bench**: JimÃ©nez, C. E., et al. (2024). \"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?\" *arXiv:2310.06770*.\n",
    "\n",
    "2. **RLAIF**: Lee, H., Phatale, S., Mansoor, H., et al. (2024). \"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.\" *Proceedings of ICML 2024*.\n",
    "\n",
    "\n",
    "### Foundational Work\n",
    "\n",
    "3. **RLHF**: Christiano, P., et al. (2017). \"Deep Reinforcement Learning from Human Preferences.\" *NeurIPS 2017*.\n",
    "\n",
    "4. **InstructGPT**: Ouyang, L., et al. (2022). \"Training Language Models to Follow Instructions with Human Feedback.\" *NeurIPS 2022*.\n",
    "\n",
    "5. **Summarization with RLHF**: Stiennon, N., et al. (2020). \"Learning to Summarize with Human Feedback.\" *NeurIPS 2020*.\n",
    "\n",
    "6. **Chain-of-Thought**: Wei, J., et al. (2022). \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\" *NeurIPS 2022*.\n",
    "\n",
    "### Evaluation Tools\n",
    "\n",
    "7. **lm-eval-harness**: Gao, L., et al. (2023). \"A Framework for Few-Shot Language Model Evaluation.\" [GitHub Repository](https://github.com/EleutherAI/lm-evaluation-harness)\n",
    "\n",
    "8. **HELM**: Liang, P., et al. (2022). \"Holistic Evaluation of Language Models.\" *arXiv:2211.09110*.\n",
    "\n",
    "9. **Epoch AI Benchmarks**: [https://epoch.ai/benchmarks](https://epoch.ai/benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Thank You!\n",
    "\n",
    "### Questions?\n",
    "\n",
    "**Contact:**\n",
    "- Chris Zhu: chris.zhu@yale.edu\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
