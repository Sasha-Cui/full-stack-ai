@inproceedings{raypaper,
author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
title = {Ray: a distributed framework for emerging AI applications},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
pages = {561â€“577},
numpages = {17},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}

@misc{rayarchitecture,
  author = {Ray Team},
  title = {Ray v2 Architecture Whitepaper},
 howpublished = {Available online: \href{https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview?tab=t.0\#heading=h.iyrm5j2gcdoq}{Ray Architecture Whitepaper}},
  urldate = {2025-10-21},
}

@article{kaplan2020,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@misc{openrouter,
  title={OpenRouter: Unified LLM API},
  author={OpenRouter Team},
  howpublished={\url{https://openrouter.ai}},
  year={2024}
}

@misc{mcp,
  title={Model Context Protocol},
  author={Anthropic},
  howpublished={\url{https://modelcontextprotocol.io}},
  year={2024}
}

@misc{openai_spec,
  title={Model Spec: Desired Behavior for OpenAI Models},
  author={OpenAI},
  howpublished={\url{https://cdn.openai.com/spec/model-spec-2024-05-08.html}},
  year={2024}
}

@article{gepa,
  title={Gradient-free Prompt Evolution Algorithm for Large Language Models},
  author={Zhang, Yuxuan and others},
  journal={arXiv preprint arXiv:2507.19457},
  year={2025}
}

@article{rlhf,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{constitutional,
  title={Constitutional AI: Harmlessness from AI feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}